{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f925de9d",
   "metadata": {},
   "source": [
    "# Repulsive Energy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a708",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "513b31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tbmalt.physics.dftb.feeds import RepulsiveSplineFeed\n",
    "from tbmalt.ml.loss_function import Loss, mse_loss\n",
    "\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "# This must be set until typecasting from HDF5 databases has been implemented.\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc005e02",
   "metadata": {},
   "source": [
    "## 1.2 Setting up the molecular systems for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "3eff86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference of target properties\n",
    "targets = {'repulsive_energy': torch.tensor([0.0727])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd751d4",
   "metadata": {},
   "source": [
    "## 1.3 Setting up the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3845c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of training cycles\n",
    "number_of_epochs = 250\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.002\n",
    "\n",
    "# Loss function\n",
    "loss_func = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8b5627f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import warnings\n",
    "# from itertools import combinations_with_replacement # No longer needed for from_database\n",
    "from typing import List, Literal, Optional, Dict, Tuple, Union, Type\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tbmalt import Geometry\n",
    "# from tbmalt.io.skf import Skf # No longer needed for Skf.RSpline\n",
    "from tbmalt.ml import Feed\n",
    "\n",
    "class RepulsiveSplineFeed(Feed):\n",
    "    r\"\"\"Repulsive Feed for DFTB-like calculations using a custom pairwise potential.\n",
    "\n",
    "    This class calculates repulsive energy based on effective nuclear charges (Z_eff)\n",
    "    and parameters (alpha) for pairs of atoms.\n",
    "    The original spline-based calculation is replaced by the new formula.\n",
    "\n",
    "    Arguments:\n",
    "        Z_eff_map: Dictionary mapping atomic numbers (int) to their effective\n",
    "                   nuclear charge (float or Tensor).\n",
    "        alpha_map: Dictionary mapping atomic numbers (int) to their alpha\n",
    "                   parameter (float or Tensor).\n",
    "        device: Device on which the feed object and its contents reside.\n",
    "        dtype: Floating point dtype to be used for tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 Z_eff_map: Dict[int, Union[float, Tensor]], \n",
    "                 alpha_map: Dict[int, Union[float, Tensor]],\n",
    "                 device: Optional[torch.device] = None,\n",
    "                 dtype: Optional[torch.dtype] = torch.float64):\n",
    "        super().__init__()\n",
    "\n",
    "        self._device = device if device is not None else torch.device('cpu')\n",
    "        self._dtype = dtype if dtype is not None else torch.float64\n",
    "\n",
    "        self.Z_eff_map = {\n",
    "            k: (v if isinstance(v, Tensor) else torch.tensor(v, device=self._device, dtype=self._dtype))\n",
    "            for k, v in Z_eff_map.items()\n",
    "        }\n",
    "        self.alpha_map = {\n",
    "            k: (v if isinstance(v, Tensor) else torch.tensor(v, device=self._device, dtype=self._dtype))\n",
    "            for k, v in alpha_map.items()\n",
    "        }\n",
    "\n",
    "        # Ensure all tensors in maps are on the correct device and dtype\n",
    "        for Z_map_k in self.Z_eff_map:\n",
    "            if self.Z_eff_map[Z_map_k].device != self._device or self.Z_eff_map[Z_map_k].dtype != self._dtype:\n",
    "                self.Z_eff_map[Z_map_k] = self.Z_eff_map[Z_map_k].to(device=self._device, dtype=self._dtype)\n",
    "        for alpha_map_k in self.alpha_map:\n",
    "            if self.alpha_map[alpha_map_k].device != self._device or self.alpha_map[alpha_map_k].dtype != self._dtype:\n",
    "                 self.alpha_map[alpha_map_k] = self.alpha_map[alpha_map_k].to(device=self._device, dtype=self._dtype)\n",
    "\n",
    "\n",
    "        warnings.warn(\n",
    "            \"The `RepulsiveSplineFeed` class is now deprecated and will be \"\n",
    "            \"removed. Its repulsive calculation logic has been changed from splines \"\n",
    "            \"to a custom pairwise potential. Consider using a more appropriately named \"\n",
    "            \"class or `PairwiseRepulsiveEnergyFeed` with a custom potential function.\",\n",
    "            category=DeprecationWarning)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        \"\"\"Floating point dtype used by `RepulsiveSplineFeed` object.\"\"\"\n",
    "        return self._dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"The device on which the `RepulsiveSplineFeed` object resides.\"\"\"\n",
    "        return self._device\n",
    "\n",
    "    def __call__(self, geo: Geometry) -> Tensor:\n",
    "        r\"\"\"Calculate the repulsive energy of a Geometry.\n",
    "\n",
    "        Arguments:\n",
    "            geo: `Geometry` object representing the system, or batch thereof,\n",
    "                for which the repulsive energy should be calculated.\n",
    "\n",
    "        Returns:\n",
    "            Erep: The repulsive energy of the Geometry object(s).\n",
    "        \"\"\"\n",
    "        batch_size, _, indx_pairs, _ = self._calculation_prep(geo)\n",
    "        \n",
    "        Erep = torch.zeros((batch_size), device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        for indx_pair in indx_pairs:\n",
    "            # Ensure indices are integers for geo.atomic_numbers access\n",
    "            idx0, idx1 = int(indx_pair[0]), int(indx_pair[1])\n",
    "\n",
    "            atomnum1_all = geo.atomic_numbers[..., idx0].reshape((batch_size, ))\n",
    "            atomnum2_all = geo.atomic_numbers[..., idx1].reshape((batch_size, ))\n",
    "            distance_all = geo.distances[..., idx0, idx1].reshape((batch_size, ))\n",
    "\n",
    "            for batch_indx in range(batch_size):\n",
    "                atomnum1 = int(atomnum1_all[batch_indx].item())\n",
    "                atomnum2 = int(atomnum2_all[batch_indx].item())\n",
    "                distance = distance_all[batch_indx]\n",
    "\n",
    "                if atomnum1 == 0 or atomnum2 == 0: # Skip dummy atoms\n",
    "                    continue\n",
    "                \n",
    "                # Retrieve parameters for the specific atom pair\n",
    "                try:\n",
    "                    Z_A_eff = self.Z_eff_map[atomnum1]\n",
    "                    Z_B_eff = self.Z_eff_map[atomnum2]\n",
    "                    alpha_A = self.alpha_map[atomnum1]\n",
    "                    alpha_B = self.alpha_map[atomnum2]\n",
    "                except KeyError as e:\n",
    "                    raise KeyError(f\"Missing Z_eff or alpha parameter for atomic number {e} in maps.\") from e\n",
    "\n",
    "                add_Erep = self._repulsive_calc(distance, Z_A_eff, Z_B_eff, alpha_A, alpha_B, grad=False)\n",
    "                Erep[batch_indx] += add_Erep\n",
    "        return Erep\n",
    "\n",
    "    def gradient(self, geo: Geometry) -> Tensor:\n",
    "        \"\"\"Calculate the gradient of the repulsive energy.\n",
    "\n",
    "        Arguments:\n",
    "            geo: `Geometry` object representing the system, or batch thereof,\n",
    "                for which the gradient of the repulsive energy should be\n",
    "                calculated.\n",
    "\n",
    "        returns:\n",
    "            dErep: The gradient of the repulsive energy.\n",
    "        \"\"\"\n",
    "        batch_size, _, indx_pairs, normed_distance_vectors = self._calculation_prep(geo)\n",
    "        \n",
    "        dErep = torch.zeros((batch_size, geo.atomic_numbers.size(dim=-1), 3), device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        for indx_pair in indx_pairs:\n",
    "            # Ensure indices are integers\n",
    "            idx0, idx1 = int(indx_pair[0]), int(indx_pair[1])\n",
    "\n",
    "            atomnum1_all = geo.atomic_numbers[..., idx0].reshape((batch_size, ))\n",
    "            atomnum2_all = geo.atomic_numbers[..., idx1].reshape((batch_size, ))\n",
    "            distance_all = geo.distances[..., idx0, idx1].reshape((batch_size, ))\n",
    "\n",
    "            for batch_indx in range(batch_size):\n",
    "                atomnum1 = int(atomnum1_all[batch_indx].item())\n",
    "                atomnum2 = int(atomnum2_all[batch_indx].item())\n",
    "                distance = distance_all[batch_indx]\n",
    "\n",
    "                if atomnum1 == 0 or atomnum2 == 0: # Skip dummy atoms\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    Z_A_eff = self.Z_eff_map[atomnum1]\n",
    "                    Z_B_eff = self.Z_eff_map[atomnum2]\n",
    "                    alpha_A = self.alpha_map[atomnum1]\n",
    "                    alpha_B = self.alpha_map[atomnum2]\n",
    "                except KeyError as e:\n",
    "                    raise KeyError(f\"Missing Z_eff or alpha parameter for atomic number {e} in maps.\") from e\n",
    "                \n",
    "                # Calculate dE/dR (scalar derivative)\n",
    "                scalar_grad_Erep = self._repulsive_calc(\n",
    "                    distance, Z_A_eff, Z_B_eff, alpha_A, alpha_B, grad=True)\n",
    "                \n",
    "                # Get normalized distance vector for this pair in this batch item\n",
    "                # Ensure normed_distance_vectors access is correct for batch_indx\n",
    "                norm_vec_ij = normed_distance_vectors[batch_indx, idx0, idx1]\n",
    "                norm_vec_ji = normed_distance_vectors[batch_indx, idx1, idx0] # Should be -norm_vec_ij\n",
    "\n",
    "                # Distribute gradient to atoms: dE/dr_i = (dE/dR) * (dr/dr_i) = (dE/dR) * u_ij\n",
    "                # Force on atom i is -dE/dr_i\n",
    "                # Gradient component for atom i is (dE/dR) * u_ij\n",
    "                # Gradient component for atom j is (dE/dR) * u_ji = (dE/dR) * (-u_ij)\n",
    "                dErep[batch_indx, idx0] += scalar_grad_Erep * norm_vec_ij\n",
    "                dErep[batch_indx, idx1] += scalar_grad_Erep * norm_vec_ji\n",
    "        \n",
    "        if batch_size == 1 and dErep.shape[0] == 1: # Ensure squeeze is safe\n",
    "            dErep = dErep.squeeze(0)\n",
    "        return dErep\n",
    "\n",
    "    def _calculation_prep(self, geo: Geometry\n",
    "                          ) -> Tuple[int, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Preliminaries for repulsive energy & gradient calculation.\n",
    "\n",
    "        Arguments:\n",
    "            geo: `Geometry` object representing the system, or batch thereof,\n",
    "                for which the calculation preparation steps are to be performed.\n",
    "\n",
    "        returns:\n",
    "            batch_size: The number of geometries in the batch.\n",
    "            indxs: The indices of the atoms.\n",
    "            indx_pairs: The indices of the interacting atom pairs as tuples.\n",
    "            normed_distance_vectors: The normalized distance vectors between the atoms\n",
    "        \"\"\"\n",
    "        if geo.atomic_numbers.dim() == 1: # this means it is not a batch\n",
    "            atomic_numbers_batched = geo.atomic_numbers.unsqueeze(0)\n",
    "            if geo.distances.dim() == 2: # (n_atoms, n_atoms)\n",
    "                 distances_batched = geo.distances.unsqueeze(0)\n",
    "                 distance_vectors_batched = geo.distance_vectors.unsqueeze(0)\n",
    "            else: # already (1, n_atoms, n_atoms)\n",
    "                 distances_batched = geo.distances\n",
    "                 distance_vectors_batched = geo.distance_vectors\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            atomic_numbers_batched = geo.atomic_numbers\n",
    "            distances_batched = geo.distances\n",
    "            distance_vectors_batched = geo.distance_vectors\n",
    "            batch_size = atomic_numbers_batched.size(dim=0)\n",
    "\n",
    "        n_atoms = atomic_numbers_batched.size(dim=-1)\n",
    "        indxs = torch.arange(n_atoms, device=self.device) # Use self.device\n",
    "        # combinations creates pairs like (0,1), (0,2), (1,2) etc.\n",
    "        # It's important that these indices match how geo.distances are indexed.\n",
    "        # geo.distances[..., i, j] is distance between atom i and atom j.\n",
    "        indx_pairs = torch.combinations(indxs, r=2)\n",
    "\n",
    "\n",
    "        # Handle division by zero for distances if atoms are at the same position\n",
    "        # Add a small epsilon to distances in denominator to prevent NaN in normed_distance_vectors\n",
    "        epsilon_dist = torch.tensor(1e-12, device=distances_batched.device, dtype=distances_batched.dtype)\n",
    "        # This is R_AB, shape (batch_size, n_atoms, n_atoms)\n",
    "        safe_distances = torch.max(distances_batched, epsilon_dist)\n",
    "\n",
    "        # normed_distance_vectors shape (batch_size, n_atoms, n_atoms, 3)\n",
    "        normed_distance_vectors = distance_vectors_batched / safe_distances.unsqueeze(-1)\n",
    "        \n",
    "        # Where original distance was truly zero, the vector was [0,0,0], so 0/epsilon resulted in 0.\n",
    "        # If distance_vectors was [0,0,0] and distances was 0, then 0/epsilon is 0.\n",
    "        # If distance_vectors was non-zero (should not happen if distance is 0) and distance was 0, then non-zero/epsilon.\n",
    "        # A more robust way if distances_batched can be zero:\n",
    "        # zero_dist_mask = (distances_batched == 0.0)\n",
    "        # normed_distance_vectors[zero_dist_mask.unsqueeze(-1).expand_as(distance_vectors_batched)] = 0.0\n",
    "        # However, the above max(distances_batched, epsilon_dist) handles the division part.\n",
    "        # If geo.distances[i,j] is 0, geo.distance_vectors[i,j,:] should also be [0,0,0].\n",
    "        # So [0,0,0] / epsilon is [0,0,0]. This seems fine.\n",
    "\n",
    "        # The original code had:\n",
    "        # normed_distance_vectors = geo.distance_vectors / geo.distances.unsqueeze(-1)\n",
    "        # normed_distance_vectors[normed_distance_vectors.isnan()] = 0\n",
    "        # This is also a valid way to handle it. Using max with epsilon avoids NaNs from division by zero directly.\n",
    "\n",
    "        # Reshape not strictly necessary if subsequent code uses (batch, N, N, 3) directly\n",
    "        # but keeping if other parts rely on this specific view for normed_distance_vectors\n",
    "        # The original reshape was:\n",
    "        # normed_distance_vectors = torch.reshape(\n",
    "        # normed_distance_vectors, (\n",
    "        # batch_size, normed_distance_vectors.shape[-3], # n_atoms\n",
    "        # normed_distance_vectors.shape[-2], # n_atoms\n",
    "        # normed_distance_vectors.shape[-1])) # 3\n",
    "        # This reshape is redundant if normed_distance_vectors is already (batch_size, n_atoms, n_atoms, 3)\n",
    "\n",
    "        return batch_size, indxs, indx_pairs, normed_distance_vectors\n",
    "\n",
    "    def _repulsive_calc(\n",
    "            self, \n",
    "            distance: Tensor, \n",
    "            Z_A_eff: Tensor, \n",
    "            Z_B_eff: Tensor,\n",
    "            alpha_A: Tensor, \n",
    "            alpha_B: Tensor, \n",
    "            grad: bool = False\n",
    "        ) -> Tensor:\n",
    "        \"\"\"Calculate the repulsive energy or its derivative between two atoms.\n",
    "        \n",
    "        Formula: E_rep = (Z_A_eff * Z_B_eff / R_AB) + exp(-sqrt(alpha_A * alpha_B) * R_AB^k_f)\n",
    "        where k_f = 1.5.\n",
    "\n",
    "        Arguments:\n",
    "            distance (R_AB): The distance between the two atoms.\n",
    "            Z_A_eff: Effective nuclear charge of atom A.\n",
    "            Z_B_eff: Effective nuclear charge of atom B.\n",
    "            alpha_A: Alpha parameter for atom A.\n",
    "            alpha_B: Alpha parameter for atom B.\n",
    "            grad: If True, calculate and return the derivative dE_rep/dR_AB.\n",
    "                  Otherwise, return E_rep.\n",
    "\n",
    "        Returns:\n",
    "            Repulsive energy or its derivative with respect to distance.\n",
    "        \"\"\"\n",
    "        k_f = 1.5\n",
    "        R_AB = distance # Use R_AB as the variable for clarity with the formula\n",
    "\n",
    "        # Ensure alpha parameters are non-negative for sqrt\n",
    "        # Assuming alpha values from map are already appropriate (e.g. >= 0)\n",
    "        # Add a small epsilon to R_AB in division to prevent inf/NaN if R_AB can be zero.\n",
    "        # However, standard torch division 1.0/0.0 = inf. Gradient of 1/R is -1/R^2.\n",
    "        # If R_AB is truly 0.0, energy is infinite. Gradient is also infinite.\n",
    "        # This is often the desired physical behavior at singularity.\n",
    "        # Clamping can be used if finite values are strictly needed for R_AB -> 0.\n",
    "        # For now, let torch handle potential infinities.\n",
    "        \n",
    "        # Term 1: Coulombic-like repulsion\n",
    "        term1 = (Z_A_eff * Z_B_eff) / R_AB\n",
    "\n",
    "        # Term 2: Exponential repulsion\n",
    "        sqrt_alpha_prod = torch.sqrt(alpha_A * alpha_B) # Assuming alpha_A, alpha_B >= 0\n",
    "        # R_AB ** k_f can be problematic if R_AB is negative, but distances are non-negative.\n",
    "        # If R_AB = 0, R_AB ** k_f = 0 (for k_f > 0).\n",
    "        exp_argument = -sqrt_alpha_prod * (R_AB ** k_f)\n",
    "        term2 = torch.exp(exp_argument)\n",
    "\n",
    "        if not grad:\n",
    "            repulsive_energy = term1 * term2\n",
    "            return repulsive_energy\n",
    "        else:\n",
    "            # Derivative of term1: d(C/R)/dR = -C/R^2\n",
    "            grad_term1 = - (Z_A_eff * Z_B_eff) / (R_AB ** 2)\n",
    "\n",
    "            # Derivative of term2: d(exp(-A * R^k_f))/dR\n",
    "            # = exp(-A * R^k_f) * (-A * k_f * R^(k_f-1))\n",
    "            # where A = sqrt(alpha_A * alpha_B)\n",
    "            # R_AB**(k_f - 1.0): if R_AB=0 and k_f-1=0.5, this is 0.0.\n",
    "            # If R_AB is very small, this derivative term dominates.\n",
    "            # If R_AB is 0, and k_f-1 > 0, then R_AB**(k_f-1) is 0, so grad_term2 is 0.\n",
    "            # This means at R_AB=0, only the Coulombic gradient remains ( انفجاری).\n",
    "            \n",
    "            # Check for R_AB == 0 to handle R_AB**(k_f-1) correctly\n",
    "            # if R_AB is a tensor, direct comparison might be needed with tolerance or use torch.where\n",
    "            if R_AB == 0.0: # Exact zero\n",
    "                 # grad_term1 will be -inf or inf (or nan if Z_A_eff*Z_B_eff is also 0)\n",
    "                 # grad_term2 contribution is 0 because R_AB**(k_f-1.0) = 0.0**(0.5) = 0.0\n",
    "                 grad_term2 = torch.tensor(0.0, device=R_AB.device, dtype=R_AB.dtype)\n",
    "            else:\n",
    "                 grad_term2 = term2 * (-sqrt_alpha_prod * k_f * (R_AB ** (k_f - 1.0)))\n",
    "            \n",
    "            return grad_term1 * grad_term2\n",
    "\n",
    "    @classmethod\n",
    "    def from_database(\n",
    "            cls, path: str, species: List[int],\n",
    "            dtype: Optional[torch.dtype] = None,\n",
    "            device: Optional[torch.device] = None\n",
    "            ) -> 'RepulsiveSplineFeed':\n",
    "        r\"\"\"Instantiate instance from a HDF5 database of Slater-Koster files.\n",
    "        \n",
    "        NOTE: This method is currently NOT IMPLEMENTED for the new repulsive\n",
    "        potential form. The original `from_database` was designed to load\n",
    "        spline data from SKF files. To use this class, you need to provide\n",
    "        `Z_eff_map` and `alpha_map` directly to the constructor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"from_database is not implemented for the current repulsive potential form. \"\n",
    "            \"Please provide Z_eff_map and alpha_map directly to the constructor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a7607",
   "metadata": {},
   "source": [
    "## 1.4 Input the molecular systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "39de93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometry: Geometry(OH2)\n",
      "OrbitalInfo: <tbmalt.structures.orbitalinfo.OrbitalInfo object at 0x7feff8b879d0>\n"
     ]
    }
   ],
   "source": [
    "from tbmalt import Geometry, OrbitalInfo\n",
    "shell_dict = {1: [0], 6: [0, 1], 7: [0, 1], 8: [0, 1]}\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# Construct the `Geometry` and `OrbitalInfo` objects. The former is analogous\n",
    "# to the ase.Atoms object while the latter provides information about what\n",
    "# orbitals are present and which atoms they belong to.\n",
    "geometry = Geometry(\n",
    "        torch.tensor([8,1,1], device=device),\n",
    "        torch.tensor([\n",
    "            [0.00000000, -0.71603315, -0.00000000],\n",
    "            [0.00000000, -0.14200298, 0.77844804 ],\n",
    "            [-0.00000000, -0.14200298, -0.77844804]],\n",
    "            device=device),units='a')\n",
    "\n",
    "orbs = OrbitalInfo(geometry.atomic_numbers, shell_dict)\n",
    "\n",
    "print('Geometry:', geometry)\n",
    "print('OrbitalInfo:', orbs)\n",
    "\n",
    "# Identify which species are present\n",
    "species = torch.unique(geometry.atomic_numbers)\n",
    "# Strip out padding species and convert to a standard list.\n",
    "species = species[species != 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7f993",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "140421b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_effs = {\n",
    "    1: torch.nn.Parameter(torch.tensor(1.116244)),  # Hydrogen\n",
    "    8: torch.tensor(5.171786)   # Oxygen\n",
    "}\n",
    "alphas = {\n",
    "    1: torch.tensor(2.209700),  # Hydrogen\n",
    "    8: torch.tensor(2.004253)   # Oxygen\n",
    "}\n",
    "\n",
    "parameter = z_effs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "97e45761",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([parameter], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "546dd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(9.0391e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0388e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0385e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0382e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0379e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0377e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0374e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0371e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0368e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0365e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0363e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0360e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0357e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0354e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0351e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0348e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0346e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0343e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0340e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0337e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0334e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0332e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0329e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0326e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0323e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0320e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0317e-06, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1645/672218363.py:55: DeprecationWarning: The `RepulsiveSplineFeed` class is now deprecated and will be removed. Its repulsive calculation logic has been changed from splines to a custom pairwise potential. Consider using a more appropriately named class or `PairwiseRepulsiveEnergyFeed` with a custom potential function.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(9.0315e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0312e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0309e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0306e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0303e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0301e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0298e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0295e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0292e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0289e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0286e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0284e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0281e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0278e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0275e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0272e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0270e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0267e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0264e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0261e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0258e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0255e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0253e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0250e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0247e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0244e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0241e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0239e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0236e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0233e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0230e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0227e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0225e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0222e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0219e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0216e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0213e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0210e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0208e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0205e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0202e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0199e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0196e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0194e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0191e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0188e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0185e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0182e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0179e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0177e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0174e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0171e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0168e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0165e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0163e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0160e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0157e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0154e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0151e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0149e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0146e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0143e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0140e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0137e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0134e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0132e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0129e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0126e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0123e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0120e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0118e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0115e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0112e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0109e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0106e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0104e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0101e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0098e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0095e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0092e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0089e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0087e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0084e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0081e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0078e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0075e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0073e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0070e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0067e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0064e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0061e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0059e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0056e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0053e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0050e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0047e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0045e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0042e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0039e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0036e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0033e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0030e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0028e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0025e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0022e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0019e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0016e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0014e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0011e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0008e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0005e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0002e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(9.0000e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9997e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9994e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9991e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9988e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9986e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9983e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9980e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9977e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9974e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9972e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9969e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9966e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9963e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9960e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9958e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9955e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9952e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9949e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9946e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9943e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9941e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9938e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9935e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9932e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9929e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9927e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9924e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9921e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9918e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9915e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9913e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9910e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9907e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9904e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9901e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9899e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9896e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9893e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9890e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9887e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9885e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9882e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9879e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9876e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9873e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9871e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9868e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9865e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9862e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9859e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9857e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9854e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9851e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9848e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9845e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9843e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9840e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9837e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9834e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9831e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9829e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9826e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9823e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9820e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9817e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9814e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9812e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9809e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9806e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9803e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9800e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9798e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9795e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9792e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9789e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9786e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9784e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9781e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9778e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9775e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9772e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9770e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9767e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9764e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9761e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9758e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9756e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9753e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9750e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9747e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9744e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9742e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9739e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9736e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9733e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9730e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9728e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9725e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9722e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9719e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9716e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9714e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9711e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9708e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9705e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9702e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9700e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9697e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9694e-06, grad_fn=<MeanBackward0>)\n",
      "loss: tensor(8.9691e-06, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "rep_feed = RepulsiveSplineFeed(Z_eff_map=z_effs, alpha_map=alphas, device=device, dtype=torch.float64)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    rep_energy = 2 * rep_feed(geometry)\n",
    "   \n",
    "    loss = mse_loss(rep_energy, targets['repulsive_energy'])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss:\",loss)\n",
    "    # Record loss\n",
    "    loss_list.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8e99f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelle: https://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00118\n",
    "# Hydrogen (H) GFN1-xTB: Parameters: alpha_A = 2.209700, Z_A = 1.116244\n",
    "# Oxygen (O) GFN1-xTB: Parameters: alpha_A = 2.004253, Z_A = 5.171786\n",
    "\n",
    "# E_REP_AB = Z_A^eff * Z_B^eff / R_AB + exp(-sqrt(alpha_A * alpha_B) * R_AB^k_f\n",
    "\n",
    "# R_AB -> Distance between atoms A and B \n",
    "# k_f -> Global Parameter: k_f = 3/2\n",
    "\n",
    "def _repulsive_calc_xTB(distance: Tensor, Z_A_eff: Tensor, Z_B_eff: Tensor,\n",
    "                    alpha_A: Tensor, alpha_B: Tensor, ) -> Tensor:\t\n",
    "    \"\"\"Calculate the repulsive energy between two atoms.\"\"\"\n",
    "\n",
    "    k_f = 1.5\n",
    "    R_AB = distance\n",
    "    repulsive_energy = (Z_A_eff * Z_B_eff / R_AB) * torch.exp(-torch.sqrt(alpha_A * alpha_B) * R_AB ** k_f)\n",
    "\n",
    "    return repulsive_energy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
