{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1905b8f9",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f5d15",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0e1ebb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tbmalt import Geometry, OrbitalInfo\n",
    "from tbmalt.physics.dftb import Dftb2\n",
    "from tbmalt.physics.dftb.feeds import SkFeed, SkfOccupationFeed, HubbardFeed, RepulsiveSplineFeed\n",
    "from tbmalt.common.maths.interpolation import CubicSpline\n",
    "from tbmalt.ml.loss_function import Loss, mse_loss\n",
    "import torch.nn as nn\n",
    "\n",
    "# This must be set until typecasting from HDF5 databases has been implemented.\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c1ff8",
   "metadata": {},
   "source": [
    "## 1.2 Setting up the molecular systems for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d62814fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {'total_energy': torch.tensor([-4.0779]),\n",
    "           'q_final_shells': torch.tensor([1.7342, 4.8584, 0.7037, 0.7037]),\n",
    "           'dipole':torch.tensor([ 0.0000e+00, -6.4284e-01, -3.3307e-16])\n",
    "           }\n",
    "\n",
    "# Provide information about the orbitals on each atom\n",
    "shell_dict = {1: [0], 6: [0, 1], 7: [0, 1], 8: [0, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959107e4",
   "metadata": {},
   "source": [
    "## 1.3 Setting up the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0fcc8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location at which the DFTB parameter set database is located\n",
    "parameter_db_path = 'mio.h5'\n",
    "\n",
    "# Training hyperparameters\n",
    "number_of_epochs = 1000\n",
    "lr = 0.008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f508be",
   "metadata": {},
   "source": [
    "## 1.4 Setting up the DFTB calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6b7c5",
   "metadata": {},
   "source": [
    "## 1.4.1 Input the molecular systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "32210a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# Construct the Geometry and OrbitalInfo objects\n",
    "geometry = Geometry(\n",
    "        torch.tensor([8,1,1], device=device),\n",
    "        torch.tensor([\n",
    "            [0.00000000, -0.71603315, -0.00000000],\n",
    "            [0.00000000, -0.14200298, 0.77844804 ],\n",
    "            [-0.00000000, -0.14200298, -0.77844804]],\n",
    "            device=device), units='a')\n",
    "\n",
    "orbs = OrbitalInfo(geometry.atomic_numbers, shell_dict)\n",
    "\n",
    "# Identify which species are present\n",
    "species = torch.unique(geometry.atomic_numbers)\n",
    "species = species[species != 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05419c79",
   "metadata": {},
   "source": [
    "## 1.4.2 Loading of the DFTB parameters into their associated feed objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5bb1b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the necessary feed models\n",
    "h_feed = SkFeed.from_database(parameter_db_path, species, 'hamiltonian',\n",
    "                              interpolation=CubicSpline, requires_grad_onsite=True)\n",
    "s_feed = SkFeed.from_database(parameter_db_path, species, 'overlap',\n",
    "                              interpolation=CubicSpline)\n",
    "o_feed = SkfOccupationFeed.from_database(parameter_db_path, species)\n",
    "u_feed = HubbardFeed.from_database(parameter_db_path, species)\n",
    "r_feed = RepulsiveSplineFeed.from_database(parameter_db_path, species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c102e17",
   "metadata": {},
   "source": [
    "## 1.4.3 Constructing the SCC-DFTB calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5555540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftb_calculator = Dftb2(h_feed, s_feed, o_feed, u_feed, r_feed, filling_scheme=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523de1b9",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73891004",
   "metadata": {},
   "source": [
    "## 2.1 Bypassing the nn.Module.__setattr__ method that was causing the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b59864f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_feed type: <class 'tbmalt.physics.dftb.feeds.SkFeed'>\n",
      "_on_sites type: <class 'torch.nn.modules.container.ParameterDict'>\n",
      "Original _on_sites keys: ['1', '8']\n",
      "Successfully replaced _on_sites!\n",
      "New _on_sites type: <class 'dict'>\n",
      "New _on_sites keys: ['1', '8']\n",
      "H tensor requires_grad: True\n",
      "O tensor requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "original_h = h_feed._on_sites[\"1\"].clone()\n",
    "original_o = h_feed._on_sites[\"8\"].clone()\n",
    "\n",
    "slice_h = torch.tensor([3.9000e-05, 3.9000e-05, 3.9000e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "        0.0000e+00, 0.0000e+00], dtype=torch.float64)\n",
    "slice_o = torch.tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
    "\n",
    "torch.manual_seed(4)  # Set random seed for reproducibility\n",
    "\n",
    "# SOLUTION: Work around the nn.Module restriction by using object.__setattr__\n",
    "# This bypasses PyTorch's restriction on dictionary assignment\n",
    "\n",
    "# First, let's see what type of object h_feed is and what _on_sites contains\n",
    "print(\"h_feed type:\", type(h_feed))\n",
    "print(\"_on_sites type:\", type(h_feed._on_sites))\n",
    "print(\"Original _on_sites keys:\", list(h_feed._on_sites.keys()))\n",
    "\n",
    "# Method 1: Use object.__setattr__ to bypass PyTorch's restrictions\n",
    "new_on_sites = {\n",
    "    \"1\": torch.zeros(9, requires_grad=True, dtype=torch.float64),\n",
    "    \"8\": torch.zeros(9, requires_grad=True, dtype=torch.float64)\n",
    "}\n",
    "\n",
    "# This bypasses the nn.Module.__setattr__ method that was causing the error\n",
    "object.__setattr__(h_feed, '_on_sites', new_on_sites)\n",
    "\n",
    "print(\"Successfully replaced _on_sites!\")\n",
    "print(\"New _on_sites type:\", type(h_feed._on_sites))\n",
    "print(\"New _on_sites keys:\", list(h_feed._on_sites.keys()))\n",
    "print(\"H tensor requires_grad:\", h_feed._on_sites[\"1\"].requires_grad)\n",
    "print(\"O tensor requires_grad:\", h_feed._on_sites[\"8\"].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b21c51",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6690c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 individual trainable parameters\n",
    "torch.manual_seed(2)  # Set random seed for reproducibility\n",
    "h_param = nn.Parameter(-torch.rand(1, dtype=torch.double))      # For H on-site[0]\n",
    "o_param_a = nn.Parameter(-torch.rand(1, dtype=torch.double))    # For O on-site[0]\n",
    "o_param_b = nn.Parameter(-torch.rand(1, dtype=torch.double))    # For O on-site[1:4]\n",
    "\n",
    "# Get the 3 specific parameters for optimization\n",
    "params = [h_param, o_param_a, o_param_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2d6539f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "36bbe6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: tensor(6.6180, grad_fn=<AddBackward0>)\n",
      "epoch 1\n",
      "loss: tensor(1.8559, grad_fn=<AddBackward0>)\n",
      "epoch 2\n",
      "loss: tensor(0.9793, grad_fn=<AddBackward0>)\n",
      "epoch 3\n",
      "loss: tensor(0.6701, grad_fn=<AddBackward0>)\n",
      "epoch 4\n",
      "loss: tensor(0.5431, grad_fn=<AddBackward0>)\n",
      "epoch 5\n",
      "loss: tensor(0.4817, grad_fn=<AddBackward0>)\n",
      "epoch 6\n",
      "loss: tensor(0.4459, grad_fn=<AddBackward0>)\n",
      "epoch 7\n",
      "loss: tensor(0.4209, grad_fn=<AddBackward0>)\n",
      "epoch 8\n",
      "loss: tensor(0.4012, grad_fn=<AddBackward0>)\n",
      "epoch 9\n",
      "loss: tensor(0.3846, grad_fn=<AddBackward0>)\n",
      "epoch 10\n",
      "loss: tensor(0.3701, grad_fn=<AddBackward0>)\n",
      "epoch 11\n",
      "loss: tensor(0.3573, grad_fn=<AddBackward0>)\n",
      "epoch 12\n",
      "loss: tensor(0.3459, grad_fn=<AddBackward0>)\n",
      "epoch 13\n",
      "loss: tensor(0.3356, grad_fn=<AddBackward0>)\n",
      "epoch 14\n",
      "loss: tensor(0.3263, grad_fn=<AddBackward0>)\n",
      "epoch 15\n",
      "loss: tensor(0.3179, grad_fn=<AddBackward0>)\n",
      "epoch 16\n",
      "loss: tensor(0.3101, grad_fn=<AddBackward0>)\n",
      "epoch 17\n",
      "loss: tensor(0.3030, grad_fn=<AddBackward0>)\n",
      "epoch 18\n",
      "loss: tensor(0.2964, grad_fn=<AddBackward0>)\n",
      "epoch 19\n",
      "loss: tensor(0.2903, grad_fn=<AddBackward0>)\n",
      "epoch 20\n",
      "loss: tensor(0.2846, grad_fn=<AddBackward0>)\n",
      "epoch 21\n",
      "loss: tensor(0.2793, grad_fn=<AddBackward0>)\n",
      "epoch 22\n",
      "loss: tensor(0.2742, grad_fn=<AddBackward0>)\n",
      "epoch 23\n",
      "loss: tensor(0.2695, grad_fn=<AddBackward0>)\n",
      "epoch 24\n",
      "loss: tensor(0.2650, grad_fn=<AddBackward0>)\n",
      "epoch 25\n",
      "loss: tensor(0.2607, grad_fn=<AddBackward0>)\n",
      "epoch 26\n",
      "loss: tensor(0.2566, grad_fn=<AddBackward0>)\n",
      "epoch 27\n",
      "loss: tensor(0.2526, grad_fn=<AddBackward0>)\n",
      "epoch 28\n",
      "loss: tensor(0.2488, grad_fn=<AddBackward0>)\n",
      "epoch 29\n",
      "loss: tensor(0.2452, grad_fn=<AddBackward0>)\n",
      "epoch 30\n",
      "loss: tensor(0.2417, grad_fn=<AddBackward0>)\n",
      "epoch 31\n",
      "loss: tensor(0.2383, grad_fn=<AddBackward0>)\n",
      "epoch 32\n",
      "loss: tensor(0.2350, grad_fn=<AddBackward0>)\n",
      "epoch 33\n",
      "loss: tensor(0.2318, grad_fn=<AddBackward0>)\n",
      "epoch 34\n",
      "loss: tensor(0.2287, grad_fn=<AddBackward0>)\n",
      "epoch 35\n",
      "loss: tensor(0.2257, grad_fn=<AddBackward0>)\n",
      "epoch 36\n",
      "loss: tensor(0.2227, grad_fn=<AddBackward0>)\n",
      "epoch 37\n",
      "loss: tensor(0.2198, grad_fn=<AddBackward0>)\n",
      "epoch 38\n",
      "loss: tensor(0.2170, grad_fn=<AddBackward0>)\n",
      "epoch 39\n",
      "loss: tensor(0.2142, grad_fn=<AddBackward0>)\n",
      "epoch 40\n",
      "loss: tensor(0.2115, grad_fn=<AddBackward0>)\n",
      "epoch 41\n",
      "loss: tensor(0.2088, grad_fn=<AddBackward0>)\n",
      "epoch 42\n",
      "loss: tensor(0.2062, grad_fn=<AddBackward0>)\n",
      "epoch 43\n",
      "loss: tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "epoch 44\n",
      "loss: tensor(0.2011, grad_fn=<AddBackward0>)\n",
      "epoch 45\n",
      "loss: tensor(0.1986, grad_fn=<AddBackward0>)\n",
      "epoch 46\n",
      "loss: tensor(0.1961, grad_fn=<AddBackward0>)\n",
      "epoch 47\n",
      "loss: tensor(0.1937, grad_fn=<AddBackward0>)\n",
      "epoch 48\n",
      "loss: tensor(0.1913, grad_fn=<AddBackward0>)\n",
      "epoch 49\n",
      "loss: tensor(0.1889, grad_fn=<AddBackward0>)\n",
      "epoch 50\n",
      "loss: tensor(0.1866, grad_fn=<AddBackward0>)\n",
      "epoch 51\n",
      "loss: tensor(0.1843, grad_fn=<AddBackward0>)\n",
      "epoch 52\n",
      "loss: tensor(0.1820, grad_fn=<AddBackward0>)\n",
      "epoch 53\n",
      "loss: tensor(0.1798, grad_fn=<AddBackward0>)\n",
      "epoch 54\n",
      "loss: tensor(0.1775, grad_fn=<AddBackward0>)\n",
      "epoch 55\n",
      "loss: tensor(0.1753, grad_fn=<AddBackward0>)\n",
      "epoch 56\n",
      "loss: tensor(0.1732, grad_fn=<AddBackward0>)\n",
      "epoch 57\n",
      "loss: tensor(0.1710, grad_fn=<AddBackward0>)\n",
      "epoch 58\n",
      "loss: tensor(0.1689, grad_fn=<AddBackward0>)\n",
      "epoch 59\n",
      "loss: tensor(0.1667, grad_fn=<AddBackward0>)\n",
      "epoch 60\n",
      "loss: tensor(0.1646, grad_fn=<AddBackward0>)\n",
      "epoch 61\n",
      "loss: tensor(0.1626, grad_fn=<AddBackward0>)\n",
      "epoch 62\n",
      "loss: tensor(0.1605, grad_fn=<AddBackward0>)\n",
      "epoch 63\n",
      "loss: tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "epoch 64\n",
      "loss: tensor(0.1564, grad_fn=<AddBackward0>)\n",
      "epoch 65\n",
      "loss: tensor(0.1544, grad_fn=<AddBackward0>)\n",
      "epoch 66\n",
      "loss: tensor(0.1525, grad_fn=<AddBackward0>)\n",
      "epoch 67\n",
      "loss: tensor(0.1505, grad_fn=<AddBackward0>)\n",
      "epoch 68\n",
      "loss: tensor(0.1485, grad_fn=<AddBackward0>)\n",
      "epoch 69\n",
      "loss: tensor(0.1466, grad_fn=<AddBackward0>)\n",
      "epoch 70\n",
      "loss: tensor(0.1447, grad_fn=<AddBackward0>)\n",
      "epoch 71\n",
      "loss: tensor(0.1428, grad_fn=<AddBackward0>)\n",
      "epoch 72\n",
      "loss: tensor(0.1409, grad_fn=<AddBackward0>)\n",
      "epoch 73\n",
      "loss: tensor(0.1390, grad_fn=<AddBackward0>)\n",
      "epoch 74\n",
      "loss: tensor(0.1372, grad_fn=<AddBackward0>)\n",
      "epoch 75\n",
      "loss: tensor(0.1353, grad_fn=<AddBackward0>)\n",
      "epoch 76\n",
      "loss: tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "epoch 77\n",
      "loss: tensor(0.1317, grad_fn=<AddBackward0>)\n",
      "epoch 78\n",
      "loss: tensor(0.1299, grad_fn=<AddBackward0>)\n",
      "epoch 79\n",
      "loss: tensor(0.1281, grad_fn=<AddBackward0>)\n",
      "epoch 80\n",
      "loss: tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "epoch 81\n",
      "loss: tensor(0.1246, grad_fn=<AddBackward0>)\n",
      "epoch 82\n",
      "loss: tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "epoch 83\n",
      "loss: tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "epoch 84\n",
      "loss: tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "epoch 85\n",
      "loss: tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "epoch 86\n",
      "loss: tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "epoch 87\n",
      "loss: tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "epoch 88\n",
      "loss: tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "epoch 89\n",
      "loss: tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "epoch 90\n",
      "loss: tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "epoch 91\n",
      "loss: tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "epoch 92\n",
      "loss: tensor(0.1064, grad_fn=<AddBackward0>)\n",
      "epoch 93\n",
      "loss: tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "epoch 94\n",
      "loss: tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "epoch 95\n",
      "loss: tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "epoch 96\n",
      "loss: tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "epoch 97\n",
      "loss: tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "epoch 98\n",
      "loss: tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "epoch 99\n",
      "loss: tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "epoch 100\n",
      "loss: tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "epoch 101\n",
      "loss: tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "epoch 102\n",
      "loss: tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "epoch 103\n",
      "loss: tensor(0.0899, grad_fn=<AddBackward0>)\n",
      "epoch 104\n",
      "loss: tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "epoch 105\n",
      "loss: tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "epoch 106\n",
      "loss: tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "epoch 107\n",
      "loss: tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "epoch 108\n",
      "loss: tensor(0.0831, grad_fn=<AddBackward0>)\n",
      "epoch 109\n",
      "loss: tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "epoch 110\n",
      "loss: tensor(0.0804, grad_fn=<AddBackward0>)\n",
      "epoch 111\n",
      "loss: tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "epoch 112\n",
      "loss: tensor(0.0778, grad_fn=<AddBackward0>)\n",
      "epoch 113\n",
      "loss: tensor(0.0766, grad_fn=<AddBackward0>)\n",
      "epoch 114\n",
      "loss: tensor(0.0753, grad_fn=<AddBackward0>)\n",
      "epoch 115\n",
      "loss: tensor(0.0741, grad_fn=<AddBackward0>)\n",
      "epoch 116\n",
      "loss: tensor(0.0729, grad_fn=<AddBackward0>)\n",
      "epoch 117\n",
      "loss: tensor(0.0717, grad_fn=<AddBackward0>)\n",
      "epoch 118\n",
      "loss: tensor(0.0705, grad_fn=<AddBackward0>)\n",
      "epoch 119\n",
      "loss: tensor(0.0693, grad_fn=<AddBackward0>)\n",
      "epoch 120\n",
      "loss: tensor(0.0681, grad_fn=<AddBackward0>)\n",
      "epoch 121\n",
      "loss: tensor(0.0670, grad_fn=<AddBackward0>)\n",
      "epoch 122\n",
      "loss: tensor(0.0658, grad_fn=<AddBackward0>)\n",
      "epoch 123\n",
      "loss: tensor(0.0647, grad_fn=<AddBackward0>)\n",
      "epoch 124\n",
      "loss: tensor(0.0636, grad_fn=<AddBackward0>)\n",
      "epoch 125\n",
      "loss: tensor(0.0625, grad_fn=<AddBackward0>)\n",
      "epoch 126\n",
      "loss: tensor(0.0615, grad_fn=<AddBackward0>)\n",
      "epoch 127\n",
      "loss: tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "epoch 128\n",
      "loss: tensor(0.0594, grad_fn=<AddBackward0>)\n",
      "epoch 129\n",
      "loss: tensor(0.0583, grad_fn=<AddBackward0>)\n",
      "epoch 130\n",
      "loss: tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "epoch 131\n",
      "loss: tensor(0.0563, grad_fn=<AddBackward0>)\n",
      "epoch 132\n",
      "loss: tensor(0.0554, grad_fn=<AddBackward0>)\n",
      "epoch 133\n",
      "loss: tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "epoch 134\n",
      "loss: tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "epoch 135\n",
      "loss: tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "epoch 136\n",
      "loss: tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "epoch 137\n",
      "loss: tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "epoch 138\n",
      "loss: tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "epoch 139\n",
      "loss: tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "epoch 140\n",
      "loss: tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "epoch 141\n",
      "loss: tensor(0.0472, grad_fn=<AddBackward0>)\n",
      "epoch 142\n",
      "loss: tensor(0.0463, grad_fn=<AddBackward0>)\n",
      "epoch 143\n",
      "loss: tensor(0.0455, grad_fn=<AddBackward0>)\n",
      "epoch 144\n",
      "loss: tensor(0.0447, grad_fn=<AddBackward0>)\n",
      "epoch 145\n",
      "loss: tensor(0.0439, grad_fn=<AddBackward0>)\n",
      "epoch 146\n",
      "loss: tensor(0.0431, grad_fn=<AddBackward0>)\n",
      "epoch 147\n",
      "loss: tensor(0.0423, grad_fn=<AddBackward0>)\n",
      "epoch 148\n",
      "loss: tensor(0.0415, grad_fn=<AddBackward0>)\n",
      "epoch 149\n",
      "loss: tensor(0.0408, grad_fn=<AddBackward0>)\n",
      "epoch 150\n",
      "loss: tensor(0.0400, grad_fn=<AddBackward0>)\n",
      "epoch 151\n",
      "loss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
      "epoch 152\n",
      "loss: tensor(0.0386, grad_fn=<AddBackward0>)\n",
      "epoch 153\n",
      "loss: tensor(0.0379, grad_fn=<AddBackward0>)\n",
      "epoch 154\n",
      "loss: tensor(0.0372, grad_fn=<AddBackward0>)\n",
      "epoch 155\n",
      "loss: tensor(0.0365, grad_fn=<AddBackward0>)\n",
      "epoch 156\n",
      "loss: tensor(0.0359, grad_fn=<AddBackward0>)\n",
      "epoch 157\n",
      "loss: tensor(0.0352, grad_fn=<AddBackward0>)\n",
      "epoch 158\n",
      "loss: tensor(0.0346, grad_fn=<AddBackward0>)\n",
      "epoch 159\n",
      "loss: tensor(0.0340, grad_fn=<AddBackward0>)\n",
      "epoch 160\n",
      "loss: tensor(0.0333, grad_fn=<AddBackward0>)\n",
      "epoch 161\n",
      "loss: tensor(0.0327, grad_fn=<AddBackward0>)\n",
      "epoch 162\n",
      "loss: tensor(0.0321, grad_fn=<AddBackward0>)\n",
      "epoch 163\n",
      "loss: tensor(0.0316, grad_fn=<AddBackward0>)\n",
      "epoch 164\n",
      "loss: tensor(0.0310, grad_fn=<AddBackward0>)\n",
      "epoch 165\n",
      "loss: tensor(0.0304, grad_fn=<AddBackward0>)\n",
      "epoch 166\n",
      "loss: tensor(0.0299, grad_fn=<AddBackward0>)\n",
      "epoch 167\n",
      "loss: tensor(0.0293, grad_fn=<AddBackward0>)\n",
      "epoch 168\n",
      "loss: tensor(0.0288, grad_fn=<AddBackward0>)\n",
      "epoch 169\n",
      "loss: tensor(0.0283, grad_fn=<AddBackward0>)\n",
      "epoch 170\n",
      "loss: tensor(0.0277, grad_fn=<AddBackward0>)\n",
      "epoch 171\n",
      "loss: tensor(0.0272, grad_fn=<AddBackward0>)\n",
      "epoch 172\n",
      "loss: tensor(0.0267, grad_fn=<AddBackward0>)\n",
      "epoch 173\n",
      "loss: tensor(0.0263, grad_fn=<AddBackward0>)\n",
      "epoch 174\n",
      "loss: tensor(0.0258, grad_fn=<AddBackward0>)\n",
      "epoch 175\n",
      "loss: tensor(0.0253, grad_fn=<AddBackward0>)\n",
      "epoch 176\n",
      "loss: tensor(0.0249, grad_fn=<AddBackward0>)\n",
      "epoch 177\n",
      "loss: tensor(0.0244, grad_fn=<AddBackward0>)\n",
      "epoch 178\n",
      "loss: tensor(0.0240, grad_fn=<AddBackward0>)\n",
      "epoch 179\n",
      "loss: tensor(0.0235, grad_fn=<AddBackward0>)\n",
      "epoch 180\n",
      "loss: tensor(0.0231, grad_fn=<AddBackward0>)\n",
      "epoch 181\n",
      "loss: tensor(0.0227, grad_fn=<AddBackward0>)\n",
      "epoch 182\n",
      "loss: tensor(0.0223, grad_fn=<AddBackward0>)\n",
      "epoch 183\n",
      "loss: tensor(0.0219, grad_fn=<AddBackward0>)\n",
      "epoch 184\n",
      "loss: tensor(0.0215, grad_fn=<AddBackward0>)\n",
      "epoch 185\n",
      "loss: tensor(0.0211, grad_fn=<AddBackward0>)\n",
      "epoch 186\n",
      "loss: tensor(0.0207, grad_fn=<AddBackward0>)\n",
      "epoch 187\n",
      "loss: tensor(0.0204, grad_fn=<AddBackward0>)\n",
      "epoch 188\n",
      "loss: tensor(0.0200, grad_fn=<AddBackward0>)\n",
      "epoch 189\n",
      "loss: tensor(0.0196, grad_fn=<AddBackward0>)\n",
      "epoch 190\n",
      "loss: tensor(0.0193, grad_fn=<AddBackward0>)\n",
      "epoch 191\n",
      "loss: tensor(0.0189, grad_fn=<AddBackward0>)\n",
      "epoch 192\n",
      "loss: tensor(0.0186, grad_fn=<AddBackward0>)\n",
      "epoch 193\n",
      "loss: tensor(0.0183, grad_fn=<AddBackward0>)\n",
      "epoch 194\n",
      "loss: tensor(0.0180, grad_fn=<AddBackward0>)\n",
      "epoch 195\n",
      "loss: tensor(0.0176, grad_fn=<AddBackward0>)\n",
      "epoch 196\n",
      "loss: tensor(0.0173, grad_fn=<AddBackward0>)\n",
      "epoch 197\n",
      "loss: tensor(0.0170, grad_fn=<AddBackward0>)\n",
      "epoch 198\n",
      "loss: tensor(0.0167, grad_fn=<AddBackward0>)\n",
      "epoch 199\n",
      "loss: tensor(0.0164, grad_fn=<AddBackward0>)\n",
      "epoch 200\n",
      "loss: tensor(0.0161, grad_fn=<AddBackward0>)\n",
      "epoch 201\n",
      "loss: tensor(0.0159, grad_fn=<AddBackward0>)\n",
      "epoch 202\n",
      "loss: tensor(0.0156, grad_fn=<AddBackward0>)\n",
      "epoch 203\n",
      "loss: tensor(0.0153, grad_fn=<AddBackward0>)\n",
      "epoch 204\n",
      "loss: tensor(0.0151, grad_fn=<AddBackward0>)\n",
      "epoch 205\n",
      "loss: tensor(0.0148, grad_fn=<AddBackward0>)\n",
      "epoch 206\n",
      "loss: tensor(0.0145, grad_fn=<AddBackward0>)\n",
      "epoch 207\n",
      "loss: tensor(0.0143, grad_fn=<AddBackward0>)\n",
      "epoch 208\n",
      "loss: tensor(0.0140, grad_fn=<AddBackward0>)\n",
      "epoch 209\n",
      "loss: tensor(0.0138, grad_fn=<AddBackward0>)\n",
      "epoch 210\n",
      "loss: tensor(0.0136, grad_fn=<AddBackward0>)\n",
      "epoch 211\n",
      "loss: tensor(0.0133, grad_fn=<AddBackward0>)\n",
      "epoch 212\n",
      "loss: tensor(0.0131, grad_fn=<AddBackward0>)\n",
      "epoch 213\n",
      "loss: tensor(0.0129, grad_fn=<AddBackward0>)\n",
      "epoch 214\n",
      "loss: tensor(0.0127, grad_fn=<AddBackward0>)\n",
      "epoch 215\n",
      "loss: tensor(0.0124, grad_fn=<AddBackward0>)\n",
      "epoch 216\n",
      "loss: tensor(0.0122, grad_fn=<AddBackward0>)\n",
      "epoch 217\n",
      "loss: tensor(0.0120, grad_fn=<AddBackward0>)\n",
      "epoch 218\n",
      "loss: tensor(0.0118, grad_fn=<AddBackward0>)\n",
      "epoch 219\n",
      "loss: tensor(0.0116, grad_fn=<AddBackward0>)\n",
      "epoch 220\n",
      "loss: tensor(0.0114, grad_fn=<AddBackward0>)\n",
      "epoch 221\n",
      "loss: tensor(0.0112, grad_fn=<AddBackward0>)\n",
      "epoch 222\n",
      "loss: tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "epoch 223\n",
      "loss: tensor(0.0109, grad_fn=<AddBackward0>)\n",
      "epoch 224\n",
      "loss: tensor(0.0107, grad_fn=<AddBackward0>)\n",
      "epoch 225\n",
      "loss: tensor(0.0105, grad_fn=<AddBackward0>)\n",
      "epoch 226\n",
      "loss: tensor(0.0103, grad_fn=<AddBackward0>)\n",
      "epoch 227\n",
      "loss: tensor(0.0102, grad_fn=<AddBackward0>)\n",
      "epoch 228\n",
      "loss: tensor(0.0100, grad_fn=<AddBackward0>)\n",
      "epoch 229\n",
      "loss: tensor(0.0098, grad_fn=<AddBackward0>)\n",
      "epoch 230\n",
      "loss: tensor(0.0097, grad_fn=<AddBackward0>)\n",
      "epoch 231\n",
      "loss: tensor(0.0095, grad_fn=<AddBackward0>)\n",
      "epoch 232\n",
      "loss: tensor(0.0094, grad_fn=<AddBackward0>)\n",
      "epoch 233\n",
      "loss: tensor(0.0092, grad_fn=<AddBackward0>)\n",
      "epoch 234\n",
      "loss: tensor(0.0091, grad_fn=<AddBackward0>)\n",
      "epoch 235\n",
      "loss: tensor(0.0089, grad_fn=<AddBackward0>)\n",
      "epoch 236\n",
      "loss: tensor(0.0088, grad_fn=<AddBackward0>)\n",
      "epoch 237\n",
      "loss: tensor(0.0086, grad_fn=<AddBackward0>)\n",
      "epoch 238\n",
      "loss: tensor(0.0085, grad_fn=<AddBackward0>)\n",
      "epoch 239\n",
      "loss: tensor(0.0084, grad_fn=<AddBackward0>)\n",
      "epoch 240\n",
      "loss: tensor(0.0082, grad_fn=<AddBackward0>)\n",
      "epoch 241\n",
      "loss: tensor(0.0081, grad_fn=<AddBackward0>)\n",
      "epoch 242\n",
      "loss: tensor(0.0080, grad_fn=<AddBackward0>)\n",
      "epoch 243\n",
      "loss: tensor(0.0078, grad_fn=<AddBackward0>)\n",
      "epoch 244\n",
      "loss: tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "epoch 245\n",
      "loss: tensor(0.0076, grad_fn=<AddBackward0>)\n",
      "epoch 246\n",
      "loss: tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "epoch 247\n",
      "loss: tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "epoch 248\n",
      "loss: tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "epoch 249\n",
      "loss: tensor(0.0071, grad_fn=<AddBackward0>)\n",
      "epoch 250\n",
      "loss: tensor(0.0070, grad_fn=<AddBackward0>)\n",
      "epoch 251\n",
      "loss: tensor(0.0069, grad_fn=<AddBackward0>)\n",
      "epoch 252\n",
      "loss: tensor(0.0068, grad_fn=<AddBackward0>)\n",
      "epoch 253\n",
      "loss: tensor(0.0067, grad_fn=<AddBackward0>)\n",
      "epoch 254\n",
      "loss: tensor(0.0066, grad_fn=<AddBackward0>)\n",
      "epoch 255\n",
      "loss: tensor(0.0065, grad_fn=<AddBackward0>)\n",
      "epoch 256\n",
      "loss: tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "epoch 257\n",
      "loss: tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "epoch 258\n",
      "loss: tensor(0.0062, grad_fn=<AddBackward0>)\n",
      "epoch 259\n",
      "loss: tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "epoch 260\n",
      "loss: tensor(0.0060, grad_fn=<AddBackward0>)\n",
      "epoch 261\n",
      "loss: tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "epoch 262\n",
      "loss: tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "epoch 263\n",
      "loss: tensor(0.0057, grad_fn=<AddBackward0>)\n",
      "epoch 264\n",
      "loss: tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "epoch 265\n",
      "loss: tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "epoch 266\n",
      "loss: tensor(0.0055, grad_fn=<AddBackward0>)\n",
      "epoch 267\n",
      "loss: tensor(0.0054, grad_fn=<AddBackward0>)\n",
      "epoch 268\n",
      "loss: tensor(0.0053, grad_fn=<AddBackward0>)\n",
      "epoch 269\n",
      "loss: tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "epoch 270\n",
      "loss: tensor(0.0051, grad_fn=<AddBackward0>)\n",
      "epoch 271\n",
      "loss: tensor(0.0051, grad_fn=<AddBackward0>)\n",
      "epoch 272\n",
      "loss: tensor(0.0050, grad_fn=<AddBackward0>)\n",
      "epoch 273\n",
      "loss: tensor(0.0049, grad_fn=<AddBackward0>)\n",
      "epoch 274\n",
      "loss: tensor(0.0048, grad_fn=<AddBackward0>)\n",
      "epoch 275\n",
      "loss: tensor(0.0048, grad_fn=<AddBackward0>)\n",
      "epoch 276\n",
      "loss: tensor(0.0047, grad_fn=<AddBackward0>)\n",
      "epoch 277\n",
      "loss: tensor(0.0046, grad_fn=<AddBackward0>)\n",
      "epoch 278\n",
      "loss: tensor(0.0046, grad_fn=<AddBackward0>)\n",
      "epoch 279\n",
      "loss: tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "epoch 280\n",
      "loss: tensor(0.0044, grad_fn=<AddBackward0>)\n",
      "epoch 281\n",
      "loss: tensor(0.0044, grad_fn=<AddBackward0>)\n",
      "epoch 282\n",
      "loss: tensor(0.0043, grad_fn=<AddBackward0>)\n",
      "epoch 283\n",
      "loss: tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "epoch 284\n",
      "loss: tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "epoch 285\n",
      "loss: tensor(0.0041, grad_fn=<AddBackward0>)\n",
      "epoch 286\n",
      "loss: tensor(0.0041, grad_fn=<AddBackward0>)\n",
      "epoch 287\n",
      "loss: tensor(0.0040, grad_fn=<AddBackward0>)\n",
      "epoch 288\n",
      "loss: tensor(0.0039, grad_fn=<AddBackward0>)\n",
      "epoch 289\n",
      "loss: tensor(0.0039, grad_fn=<AddBackward0>)\n",
      "epoch 290\n",
      "loss: tensor(0.0038, grad_fn=<AddBackward0>)\n",
      "epoch 291\n",
      "loss: tensor(0.0038, grad_fn=<AddBackward0>)\n",
      "epoch 292\n",
      "loss: tensor(0.0037, grad_fn=<AddBackward0>)\n",
      "epoch 293\n",
      "loss: tensor(0.0037, grad_fn=<AddBackward0>)\n",
      "epoch 294\n",
      "loss: tensor(0.0036, grad_fn=<AddBackward0>)\n",
      "epoch 295\n",
      "loss: tensor(0.0036, grad_fn=<AddBackward0>)\n",
      "epoch 296\n",
      "loss: tensor(0.0035, grad_fn=<AddBackward0>)\n",
      "epoch 297\n",
      "loss: tensor(0.0035, grad_fn=<AddBackward0>)\n",
      "epoch 298\n",
      "loss: tensor(0.0034, grad_fn=<AddBackward0>)\n",
      "epoch 299\n",
      "loss: tensor(0.0034, grad_fn=<AddBackward0>)\n",
      "epoch 300\n",
      "loss: tensor(0.0033, grad_fn=<AddBackward0>)\n",
      "epoch 301\n",
      "loss: tensor(0.0033, grad_fn=<AddBackward0>)\n",
      "epoch 302\n",
      "loss: tensor(0.0032, grad_fn=<AddBackward0>)\n",
      "epoch 303\n",
      "loss: tensor(0.0032, grad_fn=<AddBackward0>)\n",
      "epoch 304\n",
      "loss: tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "epoch 305\n",
      "loss: tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "epoch 306\n",
      "loss: tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "epoch 307\n",
      "loss: tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "epoch 308\n",
      "loss: tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "epoch 309\n",
      "loss: tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "epoch 310\n",
      "loss: tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "epoch 311\n",
      "loss: tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "epoch 312\n",
      "loss: tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "epoch 313\n",
      "loss: tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "epoch 314\n",
      "loss: tensor(0.0027, grad_fn=<AddBackward0>)\n",
      "epoch 315\n",
      "loss: tensor(0.0027, grad_fn=<AddBackward0>)\n",
      "epoch 316\n",
      "loss: tensor(0.0026, grad_fn=<AddBackward0>)\n",
      "epoch 317\n",
      "loss: tensor(0.0026, grad_fn=<AddBackward0>)\n",
      "epoch 318\n",
      "loss: tensor(0.0026, grad_fn=<AddBackward0>)\n",
      "epoch 319\n",
      "loss: tensor(0.0025, grad_fn=<AddBackward0>)\n",
      "epoch 320\n",
      "loss: tensor(0.0025, grad_fn=<AddBackward0>)\n",
      "epoch 321\n",
      "loss: tensor(0.0025, grad_fn=<AddBackward0>)\n",
      "epoch 322\n",
      "loss: tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "epoch 323\n",
      "loss: tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "epoch 324\n",
      "loss: tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "epoch 325\n",
      "loss: tensor(0.0023, grad_fn=<AddBackward0>)\n",
      "epoch 326\n",
      "loss: tensor(0.0023, grad_fn=<AddBackward0>)\n",
      "epoch 327\n",
      "loss: tensor(0.0023, grad_fn=<AddBackward0>)\n",
      "epoch 328\n",
      "loss: tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "epoch 329\n",
      "loss: tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "epoch 330\n",
      "loss: tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "epoch 331\n",
      "loss: tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "epoch 332\n",
      "loss: tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "epoch 333\n",
      "loss: tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "epoch 334\n",
      "loss: tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "epoch 335\n",
      "loss: tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "epoch 336\n",
      "loss: tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "epoch 337\n",
      "loss: tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "epoch 338\n",
      "loss: tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "epoch 339\n",
      "loss: tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "epoch 340\n",
      "loss: tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "epoch 341\n",
      "loss: tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "epoch 342\n",
      "loss: tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "epoch 343\n",
      "loss: tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "epoch 344\n",
      "loss: tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "epoch 345\n",
      "loss: tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "epoch 346\n",
      "loss: tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "epoch 347\n",
      "loss: tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "epoch 348\n",
      "loss: tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "epoch 349\n",
      "loss: tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "epoch 350\n",
      "loss: tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "epoch 351\n",
      "loss: tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "epoch 352\n",
      "loss: tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "epoch 353\n",
      "loss: tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "epoch 354\n",
      "loss: tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "epoch 355\n",
      "loss: tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "epoch 356\n",
      "loss: tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch 357\n",
      "loss: tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch 358\n",
      "loss: tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch 359\n",
      "loss: tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch 360\n",
      "loss: tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch 361\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 362\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 363\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 364\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 365\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 366\n",
      "loss: tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "epoch 367\n",
      "loss: tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch 368\n",
      "loss: tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch 369\n",
      "loss: tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch 370\n",
      "loss: tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch 371\n",
      "loss: tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch 372\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 373\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 374\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 375\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 376\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 377\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 378\n",
      "loss: tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch 379\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 380\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 381\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 382\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 383\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 384\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 385\n",
      "loss: tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch 386\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 387\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 388\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 389\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 390\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 391\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 392\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 393\n",
      "loss: tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch 394\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 395\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 396\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 397\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 398\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 399\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 400\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 401\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 402\n",
      "loss: tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch 403\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 404\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 405\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 406\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 407\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 408\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 409\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 410\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 411\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 412\n",
      "loss: tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch 413\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 414\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 415\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 416\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 417\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 418\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 419\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 420\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 421\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 422\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 423\n",
      "loss: tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch 424\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 425\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 426\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 427\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 428\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 429\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 430\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 431\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 432\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 433\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 434\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 435\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 436\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 437\n",
      "loss: tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch 438\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 439\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 440\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 441\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 442\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 443\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 444\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 445\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 446\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 447\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 448\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 449\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 450\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 451\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 452\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 453\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 454\n",
      "loss: tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch 455\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 456\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 457\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 458\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 459\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 460\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 461\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 462\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 463\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 464\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 465\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 466\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 467\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 468\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 469\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 470\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 471\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 472\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 473\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 474\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 475\n",
      "loss: tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch 476\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 477\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 478\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 479\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 480\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 481\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 482\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 483\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 484\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 485\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 486\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 487\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 488\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 489\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 490\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 491\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 492\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 493\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 494\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 495\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 496\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 497\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 498\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 499\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 500\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 501\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 502\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 503\n",
      "loss: tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch 504\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 505\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 506\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 507\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 508\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 509\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 510\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 511\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 512\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 513\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 514\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 515\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 516\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 517\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 518\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 519\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 520\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 521\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 522\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 523\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 524\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 525\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 526\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 527\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 528\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 529\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 530\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 531\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 532\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 533\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 534\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 535\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 536\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 537\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 538\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 539\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 540\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 541\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 542\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 543\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 544\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 545\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 546\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 547\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 548\n",
      "loss: tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch 549\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 550\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 551\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 552\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 553\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 554\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 555\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 556\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 557\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 558\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 559\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 560\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 561\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 562\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 563\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 564\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 565\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 566\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 567\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 568\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 569\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 570\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 571\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 572\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 573\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 574\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 575\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 576\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 577\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 578\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 579\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 580\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 581\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 582\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 583\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 584\n",
      "loss: tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "epoch 585\n",
      "loss: tensor(9.9031e-05, grad_fn=<AddBackward0>)\n",
      "epoch 586\n",
      "loss: tensor(9.7923e-05, grad_fn=<AddBackward0>)\n",
      "epoch 587\n",
      "loss: tensor(9.6828e-05, grad_fn=<AddBackward0>)\n",
      "epoch 588\n",
      "loss: tensor(9.5745e-05, grad_fn=<AddBackward0>)\n",
      "epoch 589\n",
      "loss: tensor(9.4675e-05, grad_fn=<AddBackward0>)\n",
      "epoch 590\n",
      "loss: tensor(9.3617e-05, grad_fn=<AddBackward0>)\n",
      "epoch 591\n",
      "loss: tensor(9.2572e-05, grad_fn=<AddBackward0>)\n",
      "epoch 592\n",
      "loss: tensor(9.1538e-05, grad_fn=<AddBackward0>)\n",
      "epoch 593\n",
      "loss: tensor(9.0516e-05, grad_fn=<AddBackward0>)\n",
      "epoch 594\n",
      "loss: tensor(8.9506e-05, grad_fn=<AddBackward0>)\n",
      "epoch 595\n",
      "loss: tensor(8.8508e-05, grad_fn=<AddBackward0>)\n",
      "epoch 596\n",
      "loss: tensor(8.7521e-05, grad_fn=<AddBackward0>)\n",
      "epoch 597\n",
      "loss: tensor(8.6545e-05, grad_fn=<AddBackward0>)\n",
      "epoch 598\n",
      "loss: tensor(8.5580e-05, grad_fn=<AddBackward0>)\n",
      "epoch 599\n",
      "loss: tensor(8.4627e-05, grad_fn=<AddBackward0>)\n",
      "epoch 600\n",
      "loss: tensor(8.3684e-05, grad_fn=<AddBackward0>)\n",
      "epoch 601\n",
      "loss: tensor(8.2753e-05, grad_fn=<AddBackward0>)\n",
      "epoch 602\n",
      "loss: tensor(8.1831e-05, grad_fn=<AddBackward0>)\n",
      "epoch 603\n",
      "loss: tensor(8.0921e-05, grad_fn=<AddBackward0>)\n",
      "epoch 604\n",
      "loss: tensor(8.0021e-05, grad_fn=<AddBackward0>)\n",
      "epoch 605\n",
      "loss: tensor(7.9131e-05, grad_fn=<AddBackward0>)\n",
      "epoch 606\n",
      "loss: tensor(7.8251e-05, grad_fn=<AddBackward0>)\n",
      "epoch 607\n",
      "loss: tensor(7.7381e-05, grad_fn=<AddBackward0>)\n",
      "epoch 608\n",
      "loss: tensor(7.6521e-05, grad_fn=<AddBackward0>)\n",
      "epoch 609\n",
      "loss: tensor(7.5671e-05, grad_fn=<AddBackward0>)\n",
      "epoch 610\n",
      "loss: tensor(7.4831e-05, grad_fn=<AddBackward0>)\n",
      "epoch 611\n",
      "loss: tensor(7.4000e-05, grad_fn=<AddBackward0>)\n",
      "epoch 612\n",
      "loss: tensor(7.3179e-05, grad_fn=<AddBackward0>)\n",
      "epoch 613\n",
      "loss: tensor(7.2367e-05, grad_fn=<AddBackward0>)\n",
      "epoch 614\n",
      "loss: tensor(7.1564e-05, grad_fn=<AddBackward0>)\n",
      "epoch 615\n",
      "loss: tensor(7.0771e-05, grad_fn=<AddBackward0>)\n",
      "epoch 616\n",
      "loss: tensor(6.9986e-05, grad_fn=<AddBackward0>)\n",
      "epoch 617\n",
      "loss: tensor(6.9210e-05, grad_fn=<AddBackward0>)\n",
      "epoch 618\n",
      "loss: tensor(6.8443e-05, grad_fn=<AddBackward0>)\n",
      "epoch 619\n",
      "loss: tensor(6.7685e-05, grad_fn=<AddBackward0>)\n",
      "epoch 620\n",
      "loss: tensor(6.6936e-05, grad_fn=<AddBackward0>)\n",
      "epoch 621\n",
      "loss: tensor(6.6195e-05, grad_fn=<AddBackward0>)\n",
      "epoch 622\n",
      "loss: tensor(6.5462e-05, grad_fn=<AddBackward0>)\n",
      "epoch 623\n",
      "loss: tensor(6.4738e-05, grad_fn=<AddBackward0>)\n",
      "epoch 624\n",
      "loss: tensor(6.4022e-05, grad_fn=<AddBackward0>)\n",
      "epoch 625\n",
      "loss: tensor(6.3314e-05, grad_fn=<AddBackward0>)\n",
      "epoch 626\n",
      "loss: tensor(6.2614e-05, grad_fn=<AddBackward0>)\n",
      "epoch 627\n",
      "loss: tensor(6.1922e-05, grad_fn=<AddBackward0>)\n",
      "epoch 628\n",
      "loss: tensor(6.1237e-05, grad_fn=<AddBackward0>)\n",
      "epoch 629\n",
      "loss: tensor(6.0561e-05, grad_fn=<AddBackward0>)\n",
      "epoch 630\n",
      "loss: tensor(5.9892e-05, grad_fn=<AddBackward0>)\n",
      "epoch 631\n",
      "loss: tensor(5.9231e-05, grad_fn=<AddBackward0>)\n",
      "epoch 632\n",
      "loss: tensor(5.8577e-05, grad_fn=<AddBackward0>)\n",
      "epoch 633\n",
      "loss: tensor(5.7930e-05, grad_fn=<AddBackward0>)\n",
      "epoch 634\n",
      "loss: tensor(5.7291e-05, grad_fn=<AddBackward0>)\n",
      "epoch 635\n",
      "loss: tensor(5.6659e-05, grad_fn=<AddBackward0>)\n",
      "epoch 636\n",
      "loss: tensor(5.6034e-05, grad_fn=<AddBackward0>)\n",
      "epoch 637\n",
      "loss: tensor(5.5417e-05, grad_fn=<AddBackward0>)\n",
      "epoch 638\n",
      "loss: tensor(5.4806e-05, grad_fn=<AddBackward0>)\n",
      "epoch 639\n",
      "loss: tensor(5.4202e-05, grad_fn=<AddBackward0>)\n",
      "epoch 640\n",
      "loss: tensor(5.3605e-05, grad_fn=<AddBackward0>)\n",
      "epoch 641\n",
      "loss: tensor(5.3014e-05, grad_fn=<AddBackward0>)\n",
      "epoch 642\n",
      "loss: tensor(5.2431e-05, grad_fn=<AddBackward0>)\n",
      "epoch 643\n",
      "loss: tensor(5.1853e-05, grad_fn=<AddBackward0>)\n",
      "epoch 644\n",
      "loss: tensor(5.1283e-05, grad_fn=<AddBackward0>)\n",
      "epoch 645\n",
      "loss: tensor(5.0718e-05, grad_fn=<AddBackward0>)\n",
      "epoch 646\n",
      "loss: tensor(5.0160e-05, grad_fn=<AddBackward0>)\n",
      "epoch 647\n",
      "loss: tensor(4.9609e-05, grad_fn=<AddBackward0>)\n",
      "epoch 648\n",
      "loss: tensor(4.9063e-05, grad_fn=<AddBackward0>)\n",
      "epoch 649\n",
      "loss: tensor(4.8524e-05, grad_fn=<AddBackward0>)\n",
      "epoch 650\n",
      "loss: tensor(4.7991e-05, grad_fn=<AddBackward0>)\n",
      "epoch 651\n",
      "loss: tensor(4.7463e-05, grad_fn=<AddBackward0>)\n",
      "epoch 652\n",
      "loss: tensor(4.6942e-05, grad_fn=<AddBackward0>)\n",
      "epoch 653\n",
      "loss: tensor(4.6426e-05, grad_fn=<AddBackward0>)\n",
      "epoch 654\n",
      "loss: tensor(4.5917e-05, grad_fn=<AddBackward0>)\n",
      "epoch 655\n",
      "loss: tensor(4.5412e-05, grad_fn=<AddBackward0>)\n",
      "epoch 656\n",
      "loss: tensor(4.4914e-05, grad_fn=<AddBackward0>)\n",
      "epoch 657\n",
      "loss: tensor(4.4421e-05, grad_fn=<AddBackward0>)\n",
      "epoch 658\n",
      "loss: tensor(4.3934e-05, grad_fn=<AddBackward0>)\n",
      "epoch 659\n",
      "loss: tensor(4.3452e-05, grad_fn=<AddBackward0>)\n",
      "epoch 660\n",
      "loss: tensor(4.2975e-05, grad_fn=<AddBackward0>)\n",
      "epoch 661\n",
      "loss: tensor(4.2504e-05, grad_fn=<AddBackward0>)\n",
      "epoch 662\n",
      "loss: tensor(4.2038e-05, grad_fn=<AddBackward0>)\n",
      "epoch 663\n",
      "loss: tensor(4.1578e-05, grad_fn=<AddBackward0>)\n",
      "epoch 664\n",
      "loss: tensor(4.1122e-05, grad_fn=<AddBackward0>)\n",
      "epoch 665\n",
      "loss: tensor(4.0672e-05, grad_fn=<AddBackward0>)\n",
      "epoch 666\n",
      "loss: tensor(4.0226e-05, grad_fn=<AddBackward0>)\n",
      "epoch 667\n",
      "loss: tensor(3.9786e-05, grad_fn=<AddBackward0>)\n",
      "epoch 668\n",
      "loss: tensor(3.9350e-05, grad_fn=<AddBackward0>)\n",
      "epoch 669\n",
      "loss: tensor(3.8920e-05, grad_fn=<AddBackward0>)\n",
      "epoch 670\n",
      "loss: tensor(3.8494e-05, grad_fn=<AddBackward0>)\n",
      "epoch 671\n",
      "loss: tensor(3.8073e-05, grad_fn=<AddBackward0>)\n",
      "epoch 672\n",
      "loss: tensor(3.7656e-05, grad_fn=<AddBackward0>)\n",
      "epoch 673\n",
      "loss: tensor(3.7244e-05, grad_fn=<AddBackward0>)\n",
      "epoch 674\n",
      "loss: tensor(3.6837e-05, grad_fn=<AddBackward0>)\n",
      "epoch 675\n",
      "loss: tensor(3.6435e-05, grad_fn=<AddBackward0>)\n",
      "epoch 676\n",
      "loss: tensor(3.6036e-05, grad_fn=<AddBackward0>)\n",
      "epoch 677\n",
      "loss: tensor(3.5643e-05, grad_fn=<AddBackward0>)\n",
      "epoch 678\n",
      "loss: tensor(3.5253e-05, grad_fn=<AddBackward0>)\n",
      "epoch 679\n",
      "loss: tensor(3.4868e-05, grad_fn=<AddBackward0>)\n",
      "epoch 680\n",
      "loss: tensor(3.4487e-05, grad_fn=<AddBackward0>)\n",
      "epoch 681\n",
      "loss: tensor(3.4111e-05, grad_fn=<AddBackward0>)\n",
      "epoch 682\n",
      "loss: tensor(3.3739e-05, grad_fn=<AddBackward0>)\n",
      "epoch 683\n",
      "loss: tensor(3.3370e-05, grad_fn=<AddBackward0>)\n",
      "epoch 684\n",
      "loss: tensor(3.3006e-05, grad_fn=<AddBackward0>)\n",
      "epoch 685\n",
      "loss: tensor(3.2646e-05, grad_fn=<AddBackward0>)\n",
      "epoch 686\n",
      "loss: tensor(3.2290e-05, grad_fn=<AddBackward0>)\n",
      "epoch 687\n",
      "loss: tensor(3.1938e-05, grad_fn=<AddBackward0>)\n",
      "epoch 688\n",
      "loss: tensor(3.1590e-05, grad_fn=<AddBackward0>)\n",
      "epoch 689\n",
      "loss: tensor(3.1245e-05, grad_fn=<AddBackward0>)\n",
      "epoch 690\n",
      "loss: tensor(3.0905e-05, grad_fn=<AddBackward0>)\n",
      "epoch 691\n",
      "loss: tensor(3.0568e-05, grad_fn=<AddBackward0>)\n",
      "epoch 692\n",
      "loss: tensor(3.0235e-05, grad_fn=<AddBackward0>)\n",
      "epoch 693\n",
      "loss: tensor(2.9905e-05, grad_fn=<AddBackward0>)\n",
      "epoch 694\n",
      "loss: tensor(2.9580e-05, grad_fn=<AddBackward0>)\n",
      "epoch 695\n",
      "loss: tensor(2.9258e-05, grad_fn=<AddBackward0>)\n",
      "epoch 696\n",
      "loss: tensor(2.8939e-05, grad_fn=<AddBackward0>)\n",
      "epoch 697\n",
      "loss: tensor(2.8624e-05, grad_fn=<AddBackward0>)\n",
      "epoch 698\n",
      "loss: tensor(2.8313e-05, grad_fn=<AddBackward0>)\n",
      "epoch 699\n",
      "loss: tensor(2.8004e-05, grad_fn=<AddBackward0>)\n",
      "epoch 700\n",
      "loss: tensor(2.7700e-05, grad_fn=<AddBackward0>)\n",
      "epoch 701\n",
      "loss: tensor(2.7399e-05, grad_fn=<AddBackward0>)\n",
      "epoch 702\n",
      "loss: tensor(2.7101e-05, grad_fn=<AddBackward0>)\n",
      "epoch 703\n",
      "loss: tensor(2.6806e-05, grad_fn=<AddBackward0>)\n",
      "epoch 704\n",
      "loss: tensor(2.6514e-05, grad_fn=<AddBackward0>)\n",
      "epoch 705\n",
      "loss: tensor(2.6226e-05, grad_fn=<AddBackward0>)\n",
      "epoch 706\n",
      "loss: tensor(2.5941e-05, grad_fn=<AddBackward0>)\n",
      "epoch 707\n",
      "loss: tensor(2.5659e-05, grad_fn=<AddBackward0>)\n",
      "epoch 708\n",
      "loss: tensor(2.5380e-05, grad_fn=<AddBackward0>)\n",
      "epoch 709\n",
      "loss: tensor(2.5105e-05, grad_fn=<AddBackward0>)\n",
      "epoch 710\n",
      "loss: tensor(2.4832e-05, grad_fn=<AddBackward0>)\n",
      "epoch 711\n",
      "loss: tensor(2.4562e-05, grad_fn=<AddBackward0>)\n",
      "epoch 712\n",
      "loss: tensor(2.4296e-05, grad_fn=<AddBackward0>)\n",
      "epoch 713\n",
      "loss: tensor(2.4032e-05, grad_fn=<AddBackward0>)\n",
      "epoch 714\n",
      "loss: tensor(2.3771e-05, grad_fn=<AddBackward0>)\n",
      "epoch 715\n",
      "loss: tensor(2.3513e-05, grad_fn=<AddBackward0>)\n",
      "epoch 716\n",
      "loss: tensor(2.3258e-05, grad_fn=<AddBackward0>)\n",
      "epoch 717\n",
      "loss: tensor(2.3006e-05, grad_fn=<AddBackward0>)\n",
      "epoch 718\n",
      "loss: tensor(2.2756e-05, grad_fn=<AddBackward0>)\n",
      "epoch 719\n",
      "loss: tensor(2.2509e-05, grad_fn=<AddBackward0>)\n",
      "epoch 720\n",
      "loss: tensor(2.2265e-05, grad_fn=<AddBackward0>)\n",
      "epoch 721\n",
      "loss: tensor(2.2024e-05, grad_fn=<AddBackward0>)\n",
      "epoch 722\n",
      "loss: tensor(2.1785e-05, grad_fn=<AddBackward0>)\n",
      "epoch 723\n",
      "loss: tensor(2.1549e-05, grad_fn=<AddBackward0>)\n",
      "epoch 724\n",
      "loss: tensor(2.1316e-05, grad_fn=<AddBackward0>)\n",
      "epoch 725\n",
      "loss: tensor(2.1085e-05, grad_fn=<AddBackward0>)\n",
      "epoch 726\n",
      "loss: tensor(2.0856e-05, grad_fn=<AddBackward0>)\n",
      "epoch 727\n",
      "loss: tensor(2.0630e-05, grad_fn=<AddBackward0>)\n",
      "epoch 728\n",
      "loss: tensor(2.0407e-05, grad_fn=<AddBackward0>)\n",
      "epoch 729\n",
      "loss: tensor(2.0186e-05, grad_fn=<AddBackward0>)\n",
      "epoch 730\n",
      "loss: tensor(1.9967e-05, grad_fn=<AddBackward0>)\n",
      "epoch 731\n",
      "loss: tensor(1.9751e-05, grad_fn=<AddBackward0>)\n",
      "epoch 732\n",
      "loss: tensor(1.9537e-05, grad_fn=<AddBackward0>)\n",
      "epoch 733\n",
      "loss: tensor(1.9326e-05, grad_fn=<AddBackward0>)\n",
      "epoch 734\n",
      "loss: tensor(1.9117e-05, grad_fn=<AddBackward0>)\n",
      "epoch 735\n",
      "loss: tensor(1.8910e-05, grad_fn=<AddBackward0>)\n",
      "epoch 736\n",
      "loss: tensor(1.8706e-05, grad_fn=<AddBackward0>)\n",
      "epoch 737\n",
      "loss: tensor(1.8503e-05, grad_fn=<AddBackward0>)\n",
      "epoch 738\n",
      "loss: tensor(1.8303e-05, grad_fn=<AddBackward0>)\n",
      "epoch 739\n",
      "loss: tensor(1.8105e-05, grad_fn=<AddBackward0>)\n",
      "epoch 740\n",
      "loss: tensor(1.7909e-05, grad_fn=<AddBackward0>)\n",
      "epoch 741\n",
      "loss: tensor(1.7716e-05, grad_fn=<AddBackward0>)\n",
      "epoch 742\n",
      "loss: tensor(1.7524e-05, grad_fn=<AddBackward0>)\n",
      "epoch 743\n",
      "loss: tensor(1.7335e-05, grad_fn=<AddBackward0>)\n",
      "epoch 744\n",
      "loss: tensor(1.7148e-05, grad_fn=<AddBackward0>)\n",
      "epoch 745\n",
      "loss: tensor(1.6963e-05, grad_fn=<AddBackward0>)\n",
      "epoch 746\n",
      "loss: tensor(1.6779e-05, grad_fn=<AddBackward0>)\n",
      "epoch 747\n",
      "loss: tensor(1.6598e-05, grad_fn=<AddBackward0>)\n",
      "epoch 748\n",
      "loss: tensor(1.6419e-05, grad_fn=<AddBackward0>)\n",
      "epoch 749\n",
      "loss: tensor(1.6242e-05, grad_fn=<AddBackward0>)\n",
      "epoch 750\n",
      "loss: tensor(1.6066e-05, grad_fn=<AddBackward0>)\n",
      "epoch 751\n",
      "loss: tensor(1.5893e-05, grad_fn=<AddBackward0>)\n",
      "epoch 752\n",
      "loss: tensor(1.5721e-05, grad_fn=<AddBackward0>)\n",
      "epoch 753\n",
      "loss: tensor(1.5552e-05, grad_fn=<AddBackward0>)\n",
      "epoch 754\n",
      "loss: tensor(1.5384e-05, grad_fn=<AddBackward0>)\n",
      "epoch 755\n",
      "loss: tensor(1.5218e-05, grad_fn=<AddBackward0>)\n",
      "epoch 756\n",
      "loss: tensor(1.5054e-05, grad_fn=<AddBackward0>)\n",
      "epoch 757\n",
      "loss: tensor(1.4891e-05, grad_fn=<AddBackward0>)\n",
      "epoch 758\n",
      "loss: tensor(1.4731e-05, grad_fn=<AddBackward0>)\n",
      "epoch 759\n",
      "loss: tensor(1.4572e-05, grad_fn=<AddBackward0>)\n",
      "epoch 760\n",
      "loss: tensor(1.4415e-05, grad_fn=<AddBackward0>)\n",
      "epoch 761\n",
      "loss: tensor(1.4259e-05, grad_fn=<AddBackward0>)\n",
      "epoch 762\n",
      "loss: tensor(1.4106e-05, grad_fn=<AddBackward0>)\n",
      "epoch 763\n",
      "loss: tensor(1.3954e-05, grad_fn=<AddBackward0>)\n",
      "epoch 764\n",
      "loss: tensor(1.3803e-05, grad_fn=<AddBackward0>)\n",
      "epoch 765\n",
      "loss: tensor(1.3654e-05, grad_fn=<AddBackward0>)\n",
      "epoch 766\n",
      "loss: tensor(1.3507e-05, grad_fn=<AddBackward0>)\n",
      "epoch 767\n",
      "loss: tensor(1.3362e-05, grad_fn=<AddBackward0>)\n",
      "epoch 768\n",
      "loss: tensor(1.3218e-05, grad_fn=<AddBackward0>)\n",
      "epoch 769\n",
      "loss: tensor(1.3076e-05, grad_fn=<AddBackward0>)\n",
      "epoch 770\n",
      "loss: tensor(1.2935e-05, grad_fn=<AddBackward0>)\n",
      "epoch 771\n",
      "loss: tensor(1.2796e-05, grad_fn=<AddBackward0>)\n",
      "epoch 772\n",
      "loss: tensor(1.2658e-05, grad_fn=<AddBackward0>)\n",
      "epoch 773\n",
      "loss: tensor(1.2522e-05, grad_fn=<AddBackward0>)\n",
      "epoch 774\n",
      "loss: tensor(1.2387e-05, grad_fn=<AddBackward0>)\n",
      "epoch 775\n",
      "loss: tensor(1.2253e-05, grad_fn=<AddBackward0>)\n",
      "epoch 776\n",
      "loss: tensor(1.2122e-05, grad_fn=<AddBackward0>)\n",
      "epoch 777\n",
      "loss: tensor(1.1991e-05, grad_fn=<AddBackward0>)\n",
      "epoch 778\n",
      "loss: tensor(1.1862e-05, grad_fn=<AddBackward0>)\n",
      "epoch 779\n",
      "loss: tensor(1.1735e-05, grad_fn=<AddBackward0>)\n",
      "epoch 780\n",
      "loss: tensor(1.1608e-05, grad_fn=<AddBackward0>)\n",
      "epoch 781\n",
      "loss: tensor(1.1483e-05, grad_fn=<AddBackward0>)\n",
      "epoch 782\n",
      "loss: tensor(1.1360e-05, grad_fn=<AddBackward0>)\n",
      "epoch 783\n",
      "loss: tensor(1.1238e-05, grad_fn=<AddBackward0>)\n",
      "epoch 784\n",
      "loss: tensor(1.1117e-05, grad_fn=<AddBackward0>)\n",
      "epoch 785\n",
      "loss: tensor(1.0998e-05, grad_fn=<AddBackward0>)\n",
      "epoch 786\n",
      "loss: tensor(1.0879e-05, grad_fn=<AddBackward0>)\n",
      "epoch 787\n",
      "loss: tensor(1.0762e-05, grad_fn=<AddBackward0>)\n",
      "epoch 788\n",
      "loss: tensor(1.0647e-05, grad_fn=<AddBackward0>)\n",
      "epoch 789\n",
      "loss: tensor(1.0532e-05, grad_fn=<AddBackward0>)\n",
      "epoch 790\n",
      "loss: tensor(1.0419e-05, grad_fn=<AddBackward0>)\n",
      "epoch 791\n",
      "loss: tensor(1.0307e-05, grad_fn=<AddBackward0>)\n",
      "epoch 792\n",
      "loss: tensor(1.0197e-05, grad_fn=<AddBackward0>)\n",
      "epoch 793\n",
      "loss: tensor(1.0087e-05, grad_fn=<AddBackward0>)\n",
      "epoch 794\n",
      "loss: tensor(9.9787e-06, grad_fn=<AddBackward0>)\n",
      "epoch 795\n",
      "loss: tensor(9.8715e-06, grad_fn=<AddBackward0>)\n",
      "epoch 796\n",
      "loss: tensor(9.7655e-06, grad_fn=<AddBackward0>)\n",
      "epoch 797\n",
      "loss: tensor(9.6607e-06, grad_fn=<AddBackward0>)\n",
      "epoch 798\n",
      "loss: tensor(9.5570e-06, grad_fn=<AddBackward0>)\n",
      "epoch 799\n",
      "loss: tensor(9.4544e-06, grad_fn=<AddBackward0>)\n",
      "epoch 800\n",
      "loss: tensor(9.3529e-06, grad_fn=<AddBackward0>)\n",
      "epoch 801\n",
      "loss: tensor(9.2526e-06, grad_fn=<AddBackward0>)\n",
      "epoch 802\n",
      "loss: tensor(9.1533e-06, grad_fn=<AddBackward0>)\n",
      "epoch 803\n",
      "loss: tensor(9.0551e-06, grad_fn=<AddBackward0>)\n",
      "epoch 804\n",
      "loss: tensor(8.9579e-06, grad_fn=<AddBackward0>)\n",
      "epoch 805\n",
      "loss: tensor(8.8618e-06, grad_fn=<AddBackward0>)\n",
      "epoch 806\n",
      "loss: tensor(8.7668e-06, grad_fn=<AddBackward0>)\n",
      "epoch 807\n",
      "loss: tensor(8.6728e-06, grad_fn=<AddBackward0>)\n",
      "epoch 808\n",
      "loss: tensor(8.5798e-06, grad_fn=<AddBackward0>)\n",
      "epoch 809\n",
      "loss: tensor(8.4878e-06, grad_fn=<AddBackward0>)\n",
      "epoch 810\n",
      "loss: tensor(8.3968e-06, grad_fn=<AddBackward0>)\n",
      "epoch 811\n",
      "loss: tensor(8.3067e-06, grad_fn=<AddBackward0>)\n",
      "epoch 812\n",
      "loss: tensor(8.2177e-06, grad_fn=<AddBackward0>)\n",
      "epoch 813\n",
      "loss: tensor(8.1296e-06, grad_fn=<AddBackward0>)\n",
      "epoch 814\n",
      "loss: tensor(8.0425e-06, grad_fn=<AddBackward0>)\n",
      "epoch 815\n",
      "loss: tensor(7.9563e-06, grad_fn=<AddBackward0>)\n",
      "epoch 816\n",
      "loss: tensor(7.8710e-06, grad_fn=<AddBackward0>)\n",
      "epoch 817\n",
      "loss: tensor(7.7867e-06, grad_fn=<AddBackward0>)\n",
      "epoch 818\n",
      "loss: tensor(7.7033e-06, grad_fn=<AddBackward0>)\n",
      "epoch 819\n",
      "loss: tensor(7.6208e-06, grad_fn=<AddBackward0>)\n",
      "epoch 820\n",
      "loss: tensor(7.5391e-06, grad_fn=<AddBackward0>)\n",
      "epoch 821\n",
      "loss: tensor(7.4584e-06, grad_fn=<AddBackward0>)\n",
      "epoch 822\n",
      "loss: tensor(7.3785e-06, grad_fn=<AddBackward0>)\n",
      "epoch 823\n",
      "loss: tensor(7.2995e-06, grad_fn=<AddBackward0>)\n",
      "epoch 824\n",
      "loss: tensor(7.2213e-06, grad_fn=<AddBackward0>)\n",
      "epoch 825\n",
      "loss: tensor(7.1440e-06, grad_fn=<AddBackward0>)\n",
      "epoch 826\n",
      "loss: tensor(7.0675e-06, grad_fn=<AddBackward0>)\n",
      "epoch 827\n",
      "loss: tensor(6.9919e-06, grad_fn=<AddBackward0>)\n",
      "epoch 828\n",
      "loss: tensor(6.9170e-06, grad_fn=<AddBackward0>)\n",
      "epoch 829\n",
      "loss: tensor(6.8430e-06, grad_fn=<AddBackward0>)\n",
      "epoch 830\n",
      "loss: tensor(6.7698e-06, grad_fn=<AddBackward0>)\n",
      "epoch 831\n",
      "loss: tensor(6.6973e-06, grad_fn=<AddBackward0>)\n",
      "epoch 832\n",
      "loss: tensor(6.6257e-06, grad_fn=<AddBackward0>)\n",
      "epoch 833\n",
      "loss: tensor(6.5548e-06, grad_fn=<AddBackward0>)\n",
      "epoch 834\n",
      "loss: tensor(6.4846e-06, grad_fn=<AddBackward0>)\n",
      "epoch 835\n",
      "loss: tensor(6.4153e-06, grad_fn=<AddBackward0>)\n",
      "epoch 836\n",
      "loss: tensor(6.3466e-06, grad_fn=<AddBackward0>)\n",
      "epoch 837\n",
      "loss: tensor(6.2788e-06, grad_fn=<AddBackward0>)\n",
      "epoch 838\n",
      "loss: tensor(6.2116e-06, grad_fn=<AddBackward0>)\n",
      "epoch 839\n",
      "loss: tensor(6.1452e-06, grad_fn=<AddBackward0>)\n",
      "epoch 840\n",
      "loss: tensor(6.0795e-06, grad_fn=<AddBackward0>)\n",
      "epoch 841\n",
      "loss: tensor(6.0145e-06, grad_fn=<AddBackward0>)\n",
      "epoch 842\n",
      "loss: tensor(5.9502e-06, grad_fn=<AddBackward0>)\n",
      "epoch 843\n",
      "loss: tensor(5.8866e-06, grad_fn=<AddBackward0>)\n",
      "epoch 844\n",
      "loss: tensor(5.8236e-06, grad_fn=<AddBackward0>)\n",
      "epoch 845\n",
      "loss: tensor(5.7614e-06, grad_fn=<AddBackward0>)\n",
      "epoch 846\n",
      "loss: tensor(5.6998e-06, grad_fn=<AddBackward0>)\n",
      "epoch 847\n",
      "loss: tensor(5.6389e-06, grad_fn=<AddBackward0>)\n",
      "epoch 848\n",
      "loss: tensor(5.5786e-06, grad_fn=<AddBackward0>)\n",
      "epoch 849\n",
      "loss: tensor(5.5190e-06, grad_fn=<AddBackward0>)\n",
      "epoch 850\n",
      "loss: tensor(5.4601e-06, grad_fn=<AddBackward0>)\n",
      "epoch 851\n",
      "loss: tensor(5.4017e-06, grad_fn=<AddBackward0>)\n",
      "epoch 852\n",
      "loss: tensor(5.3440e-06, grad_fn=<AddBackward0>)\n",
      "epoch 853\n",
      "loss: tensor(5.2870e-06, grad_fn=<AddBackward0>)\n",
      "epoch 854\n",
      "loss: tensor(5.2305e-06, grad_fn=<AddBackward0>)\n",
      "epoch 855\n",
      "loss: tensor(5.1746e-06, grad_fn=<AddBackward0>)\n",
      "epoch 856\n",
      "loss: tensor(5.1194e-06, grad_fn=<AddBackward0>)\n",
      "epoch 857\n",
      "loss: tensor(5.0647e-06, grad_fn=<AddBackward0>)\n",
      "epoch 858\n",
      "loss: tensor(5.0106e-06, grad_fn=<AddBackward0>)\n",
      "epoch 859\n",
      "loss: tensor(4.9571e-06, grad_fn=<AddBackward0>)\n",
      "epoch 860\n",
      "loss: tensor(4.9042e-06, grad_fn=<AddBackward0>)\n",
      "epoch 861\n",
      "loss: tensor(4.8518e-06, grad_fn=<AddBackward0>)\n",
      "epoch 862\n",
      "loss: tensor(4.8000e-06, grad_fn=<AddBackward0>)\n",
      "epoch 863\n",
      "loss: tensor(4.7488e-06, grad_fn=<AddBackward0>)\n",
      "epoch 864\n",
      "loss: tensor(4.6981e-06, grad_fn=<AddBackward0>)\n",
      "epoch 865\n",
      "loss: tensor(4.6480e-06, grad_fn=<AddBackward0>)\n",
      "epoch 866\n",
      "loss: tensor(4.5984e-06, grad_fn=<AddBackward0>)\n",
      "epoch 867\n",
      "loss: tensor(4.5493e-06, grad_fn=<AddBackward0>)\n",
      "epoch 868\n",
      "loss: tensor(4.5008e-06, grad_fn=<AddBackward0>)\n",
      "epoch 869\n",
      "loss: tensor(4.4528e-06, grad_fn=<AddBackward0>)\n",
      "epoch 870\n",
      "loss: tensor(4.4053e-06, grad_fn=<AddBackward0>)\n",
      "epoch 871\n",
      "loss: tensor(4.3583e-06, grad_fn=<AddBackward0>)\n",
      "epoch 872\n",
      "loss: tensor(4.3118e-06, grad_fn=<AddBackward0>)\n",
      "epoch 873\n",
      "loss: tensor(4.2658e-06, grad_fn=<AddBackward0>)\n",
      "epoch 874\n",
      "loss: tensor(4.2203e-06, grad_fn=<AddBackward0>)\n",
      "epoch 875\n",
      "loss: tensor(4.1753e-06, grad_fn=<AddBackward0>)\n",
      "epoch 876\n",
      "loss: tensor(4.1307e-06, grad_fn=<AddBackward0>)\n",
      "epoch 877\n",
      "loss: tensor(4.0867e-06, grad_fn=<AddBackward0>)\n",
      "epoch 878\n",
      "loss: tensor(4.0431e-06, grad_fn=<AddBackward0>)\n",
      "epoch 879\n",
      "loss: tensor(4.0000e-06, grad_fn=<AddBackward0>)\n",
      "epoch 880\n",
      "loss: tensor(3.9574e-06, grad_fn=<AddBackward0>)\n",
      "epoch 881\n",
      "loss: tensor(3.9152e-06, grad_fn=<AddBackward0>)\n",
      "epoch 882\n",
      "loss: tensor(3.8735e-06, grad_fn=<AddBackward0>)\n",
      "epoch 883\n",
      "loss: tensor(3.8322e-06, grad_fn=<AddBackward0>)\n",
      "epoch 884\n",
      "loss: tensor(3.7913e-06, grad_fn=<AddBackward0>)\n",
      "epoch 885\n",
      "loss: tensor(3.7509e-06, grad_fn=<AddBackward0>)\n",
      "epoch 886\n",
      "loss: tensor(3.7109e-06, grad_fn=<AddBackward0>)\n",
      "epoch 887\n",
      "loss: tensor(3.6714e-06, grad_fn=<AddBackward0>)\n",
      "epoch 888\n",
      "loss: tensor(3.6323e-06, grad_fn=<AddBackward0>)\n",
      "epoch 889\n",
      "loss: tensor(3.5936e-06, grad_fn=<AddBackward0>)\n",
      "epoch 890\n",
      "loss: tensor(3.5553e-06, grad_fn=<AddBackward0>)\n",
      "epoch 891\n",
      "loss: tensor(3.5174e-06, grad_fn=<AddBackward0>)\n",
      "epoch 892\n",
      "loss: tensor(3.4799e-06, grad_fn=<AddBackward0>)\n",
      "epoch 893\n",
      "loss: tensor(3.4429e-06, grad_fn=<AddBackward0>)\n",
      "epoch 894\n",
      "loss: tensor(3.4062e-06, grad_fn=<AddBackward0>)\n",
      "epoch 895\n",
      "loss: tensor(3.3699e-06, grad_fn=<AddBackward0>)\n",
      "epoch 896\n",
      "loss: tensor(3.3340e-06, grad_fn=<AddBackward0>)\n",
      "epoch 897\n",
      "loss: tensor(3.2985e-06, grad_fn=<AddBackward0>)\n",
      "epoch 898\n",
      "loss: tensor(3.2634e-06, grad_fn=<AddBackward0>)\n",
      "epoch 899\n",
      "loss: tensor(3.2287e-06, grad_fn=<AddBackward0>)\n",
      "epoch 900\n",
      "loss: tensor(3.1943e-06, grad_fn=<AddBackward0>)\n",
      "epoch 901\n",
      "loss: tensor(3.1603e-06, grad_fn=<AddBackward0>)\n",
      "epoch 902\n",
      "loss: tensor(3.1266e-06, grad_fn=<AddBackward0>)\n",
      "epoch 903\n",
      "loss: tensor(3.0933e-06, grad_fn=<AddBackward0>)\n",
      "epoch 904\n",
      "loss: tensor(3.0604e-06, grad_fn=<AddBackward0>)\n",
      "epoch 905\n",
      "loss: tensor(3.0278e-06, grad_fn=<AddBackward0>)\n",
      "epoch 906\n",
      "loss: tensor(2.9956e-06, grad_fn=<AddBackward0>)\n",
      "epoch 907\n",
      "loss: tensor(2.9637e-06, grad_fn=<AddBackward0>)\n",
      "epoch 908\n",
      "loss: tensor(2.9322e-06, grad_fn=<AddBackward0>)\n",
      "epoch 909\n",
      "loss: tensor(2.9010e-06, grad_fn=<AddBackward0>)\n",
      "epoch 910\n",
      "loss: tensor(2.8701e-06, grad_fn=<AddBackward0>)\n",
      "epoch 911\n",
      "loss: tensor(2.8396e-06, grad_fn=<AddBackward0>)\n",
      "epoch 912\n",
      "loss: tensor(2.8093e-06, grad_fn=<AddBackward0>)\n",
      "epoch 913\n",
      "loss: tensor(2.7795e-06, grad_fn=<AddBackward0>)\n",
      "epoch 914\n",
      "loss: tensor(2.7499e-06, grad_fn=<AddBackward0>)\n",
      "epoch 915\n",
      "loss: tensor(2.7206e-06, grad_fn=<AddBackward0>)\n",
      "epoch 916\n",
      "loss: tensor(2.6917e-06, grad_fn=<AddBackward0>)\n",
      "epoch 917\n",
      "loss: tensor(2.6631e-06, grad_fn=<AddBackward0>)\n",
      "epoch 918\n",
      "loss: tensor(2.6347e-06, grad_fn=<AddBackward0>)\n",
      "epoch 919\n",
      "loss: tensor(2.6067e-06, grad_fn=<AddBackward0>)\n",
      "epoch 920\n",
      "loss: tensor(2.5790e-06, grad_fn=<AddBackward0>)\n",
      "epoch 921\n",
      "loss: tensor(2.5516e-06, grad_fn=<AddBackward0>)\n",
      "epoch 922\n",
      "loss: tensor(2.5244e-06, grad_fn=<AddBackward0>)\n",
      "epoch 923\n",
      "loss: tensor(2.4976e-06, grad_fn=<AddBackward0>)\n",
      "epoch 924\n",
      "loss: tensor(2.4710e-06, grad_fn=<AddBackward0>)\n",
      "epoch 925\n",
      "loss: tensor(2.4448e-06, grad_fn=<AddBackward0>)\n",
      "epoch 926\n",
      "loss: tensor(2.4188e-06, grad_fn=<AddBackward0>)\n",
      "epoch 927\n",
      "loss: tensor(2.3930e-06, grad_fn=<AddBackward0>)\n",
      "epoch 928\n",
      "loss: tensor(2.3676e-06, grad_fn=<AddBackward0>)\n",
      "epoch 929\n",
      "loss: tensor(2.3424e-06, grad_fn=<AddBackward0>)\n",
      "epoch 930\n",
      "loss: tensor(2.3175e-06, grad_fn=<AddBackward0>)\n",
      "epoch 931\n",
      "loss: tensor(2.2929e-06, grad_fn=<AddBackward0>)\n",
      "epoch 932\n",
      "loss: tensor(2.2685e-06, grad_fn=<AddBackward0>)\n",
      "epoch 933\n",
      "loss: tensor(2.2444e-06, grad_fn=<AddBackward0>)\n",
      "epoch 934\n",
      "loss: tensor(2.2206e-06, grad_fn=<AddBackward0>)\n",
      "epoch 935\n",
      "loss: tensor(2.1970e-06, grad_fn=<AddBackward0>)\n",
      "epoch 936\n",
      "loss: tensor(2.1736e-06, grad_fn=<AddBackward0>)\n",
      "epoch 937\n",
      "loss: tensor(2.1505e-06, grad_fn=<AddBackward0>)\n",
      "epoch 938\n",
      "loss: tensor(2.1277e-06, grad_fn=<AddBackward0>)\n",
      "epoch 939\n",
      "loss: tensor(2.1051e-06, grad_fn=<AddBackward0>)\n",
      "epoch 940\n",
      "loss: tensor(2.0827e-06, grad_fn=<AddBackward0>)\n",
      "epoch 941\n",
      "loss: tensor(2.0606e-06, grad_fn=<AddBackward0>)\n",
      "epoch 942\n",
      "loss: tensor(2.0387e-06, grad_fn=<AddBackward0>)\n",
      "epoch 943\n",
      "loss: tensor(2.0170e-06, grad_fn=<AddBackward0>)\n",
      "epoch 944\n",
      "loss: tensor(1.9956e-06, grad_fn=<AddBackward0>)\n",
      "epoch 945\n",
      "loss: tensor(1.9744e-06, grad_fn=<AddBackward0>)\n",
      "epoch 946\n",
      "loss: tensor(1.9534e-06, grad_fn=<AddBackward0>)\n",
      "epoch 947\n",
      "loss: tensor(1.9327e-06, grad_fn=<AddBackward0>)\n",
      "epoch 948\n",
      "loss: tensor(1.9122e-06, grad_fn=<AddBackward0>)\n",
      "epoch 949\n",
      "loss: tensor(1.8919e-06, grad_fn=<AddBackward0>)\n",
      "epoch 950\n",
      "loss: tensor(1.8718e-06, grad_fn=<AddBackward0>)\n",
      "epoch 951\n",
      "loss: tensor(1.8519e-06, grad_fn=<AddBackward0>)\n",
      "epoch 952\n",
      "loss: tensor(1.8322e-06, grad_fn=<AddBackward0>)\n",
      "epoch 953\n",
      "loss: tensor(1.8128e-06, grad_fn=<AddBackward0>)\n",
      "epoch 954\n",
      "loss: tensor(1.7935e-06, grad_fn=<AddBackward0>)\n",
      "epoch 955\n",
      "loss: tensor(1.7745e-06, grad_fn=<AddBackward0>)\n",
      "epoch 956\n",
      "loss: tensor(1.7556e-06, grad_fn=<AddBackward0>)\n",
      "epoch 957\n",
      "loss: tensor(1.7370e-06, grad_fn=<AddBackward0>)\n",
      "epoch 958\n",
      "loss: tensor(1.7186e-06, grad_fn=<AddBackward0>)\n",
      "epoch 959\n",
      "loss: tensor(1.7003e-06, grad_fn=<AddBackward0>)\n",
      "epoch 960\n",
      "loss: tensor(1.6823e-06, grad_fn=<AddBackward0>)\n",
      "epoch 961\n",
      "loss: tensor(1.6644e-06, grad_fn=<AddBackward0>)\n",
      "epoch 962\n",
      "loss: tensor(1.6467e-06, grad_fn=<AddBackward0>)\n",
      "epoch 963\n",
      "loss: tensor(1.6293e-06, grad_fn=<AddBackward0>)\n",
      "epoch 964\n",
      "loss: tensor(1.6120e-06, grad_fn=<AddBackward0>)\n",
      "epoch 965\n",
      "loss: tensor(1.5949e-06, grad_fn=<AddBackward0>)\n",
      "epoch 966\n",
      "loss: tensor(1.5779e-06, grad_fn=<AddBackward0>)\n",
      "epoch 967\n",
      "loss: tensor(1.5612e-06, grad_fn=<AddBackward0>)\n",
      "epoch 968\n",
      "loss: tensor(1.5446e-06, grad_fn=<AddBackward0>)\n",
      "epoch 969\n",
      "loss: tensor(1.5282e-06, grad_fn=<AddBackward0>)\n",
      "epoch 970\n",
      "loss: tensor(1.5120e-06, grad_fn=<AddBackward0>)\n",
      "epoch 971\n",
      "loss: tensor(1.4960e-06, grad_fn=<AddBackward0>)\n",
      "epoch 972\n",
      "loss: tensor(1.4801e-06, grad_fn=<AddBackward0>)\n",
      "epoch 973\n",
      "loss: tensor(1.4644e-06, grad_fn=<AddBackward0>)\n",
      "epoch 974\n",
      "loss: tensor(1.4489e-06, grad_fn=<AddBackward0>)\n",
      "epoch 975\n",
      "loss: tensor(1.4335e-06, grad_fn=<AddBackward0>)\n",
      "epoch 976\n",
      "loss: tensor(1.4183e-06, grad_fn=<AddBackward0>)\n",
      "epoch 977\n",
      "loss: tensor(1.4033e-06, grad_fn=<AddBackward0>)\n",
      "epoch 978\n",
      "loss: tensor(1.3884e-06, grad_fn=<AddBackward0>)\n",
      "epoch 979\n",
      "loss: tensor(1.3736e-06, grad_fn=<AddBackward0>)\n",
      "epoch 980\n",
      "loss: tensor(1.3591e-06, grad_fn=<AddBackward0>)\n",
      "epoch 981\n",
      "loss: tensor(1.3447e-06, grad_fn=<AddBackward0>)\n",
      "epoch 982\n",
      "loss: tensor(1.3304e-06, grad_fn=<AddBackward0>)\n",
      "epoch 983\n",
      "loss: tensor(1.3163e-06, grad_fn=<AddBackward0>)\n",
      "epoch 984\n",
      "loss: tensor(1.3023e-06, grad_fn=<AddBackward0>)\n",
      "epoch 985\n",
      "loss: tensor(1.2885e-06, grad_fn=<AddBackward0>)\n",
      "epoch 986\n",
      "loss: tensor(1.2749e-06, grad_fn=<AddBackward0>)\n",
      "epoch 987\n",
      "loss: tensor(1.2613e-06, grad_fn=<AddBackward0>)\n",
      "epoch 988\n",
      "loss: tensor(1.2480e-06, grad_fn=<AddBackward0>)\n",
      "epoch 989\n",
      "loss: tensor(1.2347e-06, grad_fn=<AddBackward0>)\n",
      "epoch 990\n",
      "loss: tensor(1.2216e-06, grad_fn=<AddBackward0>)\n",
      "epoch 991\n",
      "loss: tensor(1.2087e-06, grad_fn=<AddBackward0>)\n",
      "epoch 992\n",
      "loss: tensor(1.1959e-06, grad_fn=<AddBackward0>)\n",
      "epoch 993\n",
      "loss: tensor(1.1832e-06, grad_fn=<AddBackward0>)\n",
      "epoch 994\n",
      "loss: tensor(1.1707e-06, grad_fn=<AddBackward0>)\n",
      "epoch 995\n",
      "loss: tensor(1.1583e-06, grad_fn=<AddBackward0>)\n",
      "epoch 996\n",
      "loss: tensor(1.1460e-06, grad_fn=<AddBackward0>)\n",
      "epoch 997\n",
      "loss: tensor(1.1338e-06, grad_fn=<AddBackward0>)\n",
      "epoch 998\n",
      "loss: tensor(1.1218e-06, grad_fn=<AddBackward0>)\n",
      "epoch 999\n",
      "loss: tensor(1.1099e-06, grad_fn=<AddBackward0>)\n",
      "==================================================\n",
      "Final Loss 1.10992462025201e-06\n",
      "Original H on-site parameters: tensor([-2.3860e-01,  3.9000e-05,  3.9000e-05,  3.9000e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "Original O on-site parameters: tensor([-0.8788, -0.3321, -0.3321, -0.3321,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], grad_fn=<CloneBackward0>)\n",
      "Final H on-site parameters: tensor([-2.3936e-01,  3.9000e-05,  3.9000e-05,  3.9000e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Final O on-site parameters: tensor([-0.8772, -0.3326, -0.3326, -0.3326,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_list = []\n",
    "for epoch in range(number_of_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    print('epoch', epoch)\n",
    "    # Now we can update the tensors as you originally intended\n",
    "    # For hydrogen (Z=1)\n",
    "    h_feed._on_sites[\"1\"] = torch.cat([h_param.view(1), slice_h])\n",
    "    \n",
    "    # For oxygen (Z=8)\n",
    "    h_feed._on_sites[\"8\"] = torch.cat([o_param_a.view(1), o_param_b.view(1).repeat(3), slice_o])\n",
    "\n",
    "    # Run DFTB calculation\n",
    "    dftb_calculator(geometry, orbs, grad_mode=\"direct\")\n",
    "    \n",
    "    # Calculate loss\n",
    "    energy_loss = mse_loss(dftb_calculator.total_energy, targets['total_energy'])\n",
    "    mulliken_loss = mse_loss(dftb_calculator.q_final_shells, targets['q_final_shells'])\n",
    "    dipole_loss = mse_loss(dftb_calculator.dipole, targets['dipole'])\n",
    "    loss = energy_loss + mulliken_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    print(\"loss:\",loss)\n",
    "    # Record loss\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "\n",
    "# Print final parameters\n",
    "print(\"=\"*50)\n",
    "print(\"Final Loss\", loss_list[-1])\n",
    "print(\"Original H on-site parameters:\", original_h)\n",
    "print(\"Original O on-site parameters:\", original_o)\n",
    "\n",
    "print(f\"Final H on-site parameters: {h_feed._on_sites['1']}\")\n",
    "print(f\"Final O on-site parameters: {h_feed._on_sites['8']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6a4a2aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAJKCAYAAAC/LK3wAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZGFJREFUeJzt3Xd4VFXi//HPTe8QQhJ6MXQUQaVIF7tYKCq4FkDxa4F13dVd3XXFxbLKuqs/e0GFFXURu1IEF8WCoEhRlyYgoRkIgUB6nfv7I+SaSWMmZDKTk/frefI49865556ZHC98OOeea9m2bQsAAAAAEJCC/N0AAAAAAEDNCG0AAAAAEMAIbQAAAAAQwAhtAAAAABDACG0AAAAAEMAIbQAAAAAQwAhtAAAAABDACG0AAAAAEMAIbQAAAAAQwAhtAACPTJ48WZ06darTsX/7299kWVb9NgiNxsiRIzVy5Eh/NwMAGi1CGwA0cpZlefSzYsUKfzfVLyZPnqyYmBh/N8Mjtm1r3rx5Gj58uJo3b66oqCidcsopuv/++5Wbm+vv5jlSU1M97nepqan+bi4ANHqWbdu2vxsBAKi71157zW371Vdf1SeffKJ58+a57T/33HOVnJxc5/MUFxfL5XIpPDzc62NLSkpUUlKiiIiIOp+/riZPnqy3335bOTk5DX5ub5SWluo3v/mNFixYoGHDhmncuHGKiorSl19+qTfeeEO9evXSf//73xP6HdaX3Nxcvffee277/vWvf2nv3r16/PHH3faPHTtWoaGhkqSwsLAGayMAmITQBgCGmT59up555hkd7/Kel5enqKioBmqV/zSW0Pbwww/rL3/5i+688049+uijbu999NFHGjNmjM477zwtWbKkQdvlaT+5+OKL9b///Y+RNQDwAaZHAkATMHLkSJ188slau3athg8frqioKP3lL3+RJH3wwQcaPXq02rRpo/DwcKWkpOiBBx5QaWmpWx2V72krnyL3z3/+Uy+++KJSUlIUHh6u/v37a82aNW7HVndPm2VZmj59ut5//32dfPLJCg8PV+/evfXxxx9Xaf+KFSt0xhlnKCIiQikpKXrhhRfq/T65t956S6effroiIyPVsmVLXXPNNdq3b59bmf3792vKlClq166dwsPD1bp1a1122WVuQeW7777T+eefr5YtWyoyMlKdO3fW9ddfX+u58/Pz9eijj6pbt256+OGHq7x/ySWXaNKkSfr444+1evVqSWUh6aSTTqq2vjPPPFNnnHGG277XXnvN+XwtWrTQxIkTtWfPHrcytfWTE1H5nrYVK1bIsiwtWLBAM2fOVNu2bRUbG6vLL79cR48eVWFhoW6//XYlJSUpJiZGU6ZMUWFhYZV6PflMAGCCEH83AADQMA4dOqQLL7xQEydO1DXXXONMs5s7d65iYmL0hz/8QTExMfr00081Y8YMZWVlVRnxqc4bb7yh7Oxs3XTTTbIsS//4xz80btw4/fzzz860uJp89dVXevfdd3XrrbcqNjZWTz75pMaPH6/du3crISFBkrR+/XpdcMEFat26tWbOnKnS0lLdf//9SkxMPPEv5Zi5c+dqypQp6t+/vx5++GEdOHBATzzxhFauXKn169erefPmkqTx48dr48aN+u1vf6tOnTopPT1dn3zyiXbv3u1sn3feeUpMTNTdd9+t5s2bKzU1Ve++++5xv4fMzEz97ne/U0hI9X80X3fddZozZ44WLlyoQYMGacKECbruuuu0Zs0a9e/f3ym3a9curV692u1399BDD+nee+/VlVdeqalTp+rgwYN66qmnNHz4cLfPJ9XcT3zh4YcfVmRkpO6++25t375dTz31lEJDQxUUFKTMzEz97W9/0+rVqzV37lx17txZM2bMqNNnAoBGzwYAGGXatGl25cv7iBEjbEn2888/X6V8Xl5elX033XSTHRUVZRcUFDj7Jk2aZHfs2NHZ3rlzpy3JTkhIsA8fPuzs/+CDD2xJ9kcffeTsu++++6q0SZIdFhZmb9++3dn3/fff25Lsp556ytl3ySWX2FFRUfa+ffucfdu2bbNDQkKq1FmdSZMm2dHR0TW+X1RUZCclJdknn3yynZ+f7+xfuHChLcmeMWOGbdu2nZmZaUuyH3300Rrreu+992xJ9po1a47bror+3//7f7Yk+7333quxzOHDh21J9rhx42zbtu2jR4/a4eHh9h133OFW7h//+IdtWZa9a9cu27ZtOzU11Q4ODrYfeught3I//vijHRIS4ra/tn5yPKNHj3brHxWNGDHCHjFihLP92Wef2ZLsk08+2S4qKnL2X3XVVbZlWfaFF17odvyZZ57pVrc3nwkATMD0SABoIsLDwzVlypQq+yMjI53X2dnZysjI0LBhw5SXl6ctW7Yct94JEyYoPj7e2R42bJgk6eeffz7useecc45SUlKc7T59+iguLs45trS0VP/97381ZswYtWnTxinXpUsXXXjhhcet3xPfffed0tPTdeutt7otlDJ69Gj16NFDixYtklT2PYWFhWnFihXKzMystq7y0Z2FCxequLjY4zZkZ2dLkmJjY2ssU/5eVlaWJCkuLk4XXnihFixY4Hb/4ptvvqlBgwapQ4cOkqR3331XLpdLV155pTIyMpyfVq1aqWvXrvrss8/czlNTP/GF6667zm00duDAgbJtu8p00oEDB2rPnj0qKSmR5P1nAoDGjtAGAE1E27Ztq129b+PGjRo7dqyaNWumuLg4JSYm6pprrpEkHT169Lj1loeDcuUBrqZgU9ux5ceXH5uenq78/Hx16dKlSrnq9tXFrl27JEndu3ev8l6PHj2c98PDwzVr1iwtWbJEycnJGj58uP7xj39o//79TvkRI0Zo/Pjxmjlzplq2bKnLLrtMc+bMqfZ+rIrKA1l5eKtOdcFuwoQJ2rNnj1atWiVJ2rFjh9auXasJEyY4ZbZt2ybbttW1a1clJia6/WzevFnp6elu56mpn/hC5d9/s2bNJEnt27evst/lcjn90dvPBACNHfe0AUATUXFErdyRI0c0YsQIxcXF6f7771dKSooiIiK0bt063XXXXXK5XMetNzg4uNr9tgeLE5/Isf5w++2365JLLtH777+vpUuX6t5779XDDz+sTz/9VP369ZNlWXr77be1evVqffTRR1q6dKmuv/56/etf/9Lq1atrfF5cz549JUk//PCDxowZU22ZH374QZLUq1cvZ98ll1yiqKgoLViwQIMHD9aCBQsUFBSkK664winjcrlkWZaWLFlS7fdduU3V9RNfqen3f7x+4e1nAoDGjtAGAE3YihUrdOjQIb377rsaPny4s3/nzp1+bNWvkpKSFBERoe3bt1d5r7p9ddGxY0dJ0tatWzVq1Ci397Zu3eq8Xy4lJUV33HGH7rjjDm3btk19+/bVv/71L7fn5Q0aNEiDBg3SQw89pDfeeENXX3215s+fr6lTp1bbhqFDh6p58+Z64403dM8991QbRF599VVJZatGlouOjtbFF1+st956S4899pjefPNNDRs2zG0qaUpKimzbVufOndWtWzcvv53AZOJnAoDaMD0SAJqw8nBQcWSrqKhIzz77rL+a5CY4OFjnnHOO3n//ff3yyy/O/u3bt9fb88rOOOMMJSUl6fnnn3ebxrhkyRJt3rxZo0ePllT2vLKCggK3Y1NSUhQbG+scl5mZWWWUsG/fvpJU6xTJqKgo3Xnnndq6davuueeeKu8vWrRIc+fO1fnnn69Bgwa5vTdhwgT98ssveumll/T999+7TY2UpHHjxik4OFgzZ86s0jbbtnXo0KEa2xWoTPxMAFAbRtoAoAkbPHiw4uPjNWnSJN12222yLEvz5s0LqOmJf/vb37Rs2TINGTJEt9xyi0pLS/X000/r5JNP1oYNGzyqo7i4WA8++GCV/S1atNCtt96qWbNmacqUKRoxYoSuuuoqZ8n/Tp066fe//70k6aefftLZZ5+tK6+8Ur169VJISIjee+89HThwQBMnTpQk/fvf/9azzz6rsWPHKiUlRdnZ2Zo9e7bi4uJ00UUX1drGu+++W+vXr9esWbO0atUqjR8/XpGRkfrqq6/02muvqWfPnvr3v/9d5biLLrpIsbGxuvPOOxUcHKzx48e7vZ+SkqIHH3xQf/7zn5WamqoxY8YoNjZWO3fu1Hvvvaf/+7//05133unR9xgoTPxMAFAbQhsANGEJCQlauHCh7rjjDv31r39VfHy8rrnmGp199tk6//zz/d08SdLpp5+uJUuW6M4779S9996r9u3b6/7779fmzZs9Wt1SKhs9vPfee6vsT0lJ0a233qrJkycrKipKjzzyiO666y5FR0dr7NixmjVrlrMiZPv27XXVVVdp+fLlmjdvnkJCQtSjRw8tWLDACUojRozQt99+q/nz5+vAgQNq1qyZBgwYoNdff12dO3eutY3BwcFasGCBXn31Vb300ku69957VVRUpJSUFN1333264447FB0dXeW4iIgIXXrppXr99dd1zjnnKCkpqUqZu+++W926ddPjjz+umTNnOp/nvPPO06WXXurRdxhoTPxMAFATyw6kf04FAMBDY8aM0caNG7Vt2zZ/NwUAAJ/injYAQMDLz8932962bZsWL16skSNH+qdBAAA0IEbaAAABr3Xr1po8ebJOOukk7dq1S88995wKCwu1fv16de3a1d/NAwDAp7inDQAQ8C644AL95z//0f79+xUeHq4zzzxTf//73wlsAIAmgZE2AAAAAAhg3NMGAAAAAAGM6ZEnwOVyKSMjQ1LZg1Ety/JziwAAAAD4g23bysvLkyS1bNlSQUH1Nz5GaDsBGRkZSk5O9nczAAAAAASQAwcOVPvczLpieiQAAAAABDBG2k5AVFSU8/rAgQOKjo72Y2uk3NxcZ+QvLS1NsbGxfm0PAh99Bt6iz8Bb9Bl4iz4DbwVKn6nYjoo5oT4Q2k5AxXvYoqOj/R7aKgq09iDw0WfgLfoMvEWfgbfoM/BWoPSZ+l7rgumRAAAAABDACG0AAAAAEMAIbQAAAAAQwAhtAAAAABDAWIjEINHR0SotLVV6enpA3ICJwEefgbfoM/AWfQbeos/AW02hzzDSBgAAAAABjNAGAAAAAAGM0AYAAAAAAYzQBgAAAAABjNAGAAAAAAGM0AYAAAAAAYzQBgAAAAABjNAGAAAAAAGM0AYAAAAAAYzQBgAAAAABjNAGAAAAAAEsxN8NQP3Z9EuWPv8pXfEhxboyKcnfzQEAAABQDxhpM8iGPUc06+OtWrL5sL+bAgAAAKCeENoMZNu2v5sAAAAAoJ4Q2gxiWWX/JbIBAAAA5iC0GcTydwMAAAAA1DtCm4EYaQMAAADMQWgziMVQGwAAAGAcQpuBWIcEAAAAMAehzSAWd7UBAAAAxiG0mYTMBgAAABiH0GYgntMGAAAAmIPQZhAG2gAAAADzENoMxDgbAAAAYA5Cm0GsY2v+MzsSAAAAMAehzSBMjwQAAADMQ2gDAAAAgABGaDOIxVAbAAAAYBxCm4G4pw0AAAAwB6HNIOUjbTbrRwIAAADGILQZxGIpEgAAAMA4hDYDMc4GAAAAmIPQZhAWIgEAAADMQ2gzEAuRAAAAAOYgtAEAAABAACO0GcRifiQAAABgHEKbgZgeCQAAAJiD0GYQxtkAAAAA8xDaDMTDtQEAAABzENoMwi1tAAAAgHkIbQbinjYAAADAHCH+boC/lJaW6tNPP9W7776rVatWaf/+/Tp06JAiIyPVrl07nXLKKRoxYoRGjBih3r17+7u5HrG4qw0AAAAwTpMMbV9//bVuvfVWff/991Xey87O1ubNm7V582YtWLBAklRcXKyQkMD/qpgeCQAAAJgn8JNIPXvhhRd0yy23yK4whzAuLk4nnXSSWrRooby8PP38889KT0/3YytPDLMjAQAAAHM0qdA2e/Zst8A2aNAg3X///Ro5cqRCQ0Pdyu7evVuLFy/WK6+80mgeWt04WgkAAADAG01mIZItW7botttucwLbtGnT9PXXX+vcc8+tEtgkqUOHDrr55pv17bffKjg4uKGbe0JYiAQAAAAwR5MZaZs6daoKCgokSeeee66efvppP7eo/pUPCPKcNgAAAMAcTWKkbfXq1Vq5cqWz/dRTT/mxNb7EBEkAAADANE0itD3//PPO6yFDhqh79+5+bI3vMT0SAAAAMEeTCG0ff/yx8/q8887zY0t8q5GslwIAAADAC8bf05aamqoDBw4426eeeqokaf/+/XrllVf03nvvaefOncrJyVFCQoJ69uyp8847TzfccIMSEhI8Pk9ubm61+6Ojo0/sAwAAAAAIKNX93b+mPFAfjA9tGzZscNtu06aNXnvtNU2fPl1Hjx51e++XX37RL7/8ouXLl+vBBx/UI488oltvvdWj8yQnJ1e7v7S0tE7trpNj8yJtW3K5XA13XjRqLpdLtm3TZ+Ax+gy8RZ+Bt+gz8FZD95mYmJgGOU8540NbRkaG2/Z7772nhx9+2Nlu06aNunTpouLiYm3atMkJctnZ2Zo2bZp2796tRx55pM7nb8iHdJe3vaS0ROnp6QoKahKzX3GCXC6Xjh49Ktu26TPwCH0G3qLPwFv0GXjL9D5jfGg7cuSI23Z5YOvWrZuee+45jRo1ynmvpKREr7/+un73u985AWjWrFnq37+/xo8fX+t50tLSqp0K2ZDTI5sfKhtpCw4OVlJSkpEdFvXP5XLJsiwlJibSZ+AR+gy8RZ+Bt+gz8FZD95msrKwq+3Jzc9W6dWufnM/40Fb+bLaKOnTooJUrV6ply5Zu+0NCQjRp0iT16tVLQ4cOVVFRkSTp7rvv1pgxY2p9yHZsbKzf718Lsso7qKWgoCAucvCYZdFn4B36DLxFn4G36DPwVkP2mdjY2Cr7fHle4/8vqC5IPfbYY1UCW0X9+/fX9OnTne3t27frs88+80n7fMFmzX8AAADAGMaHtsopuFmzZhozZsxxj5syZYrb9ooVK+qxVb7Bkv8AAACAeYwPbYmJiW7bffv2rXWaY7nevXsrIiLC2d6xY0e9t62+EdoAAAAA8xgf2nr16uW27emz1yzLUosWLZztw4cP12u7fInJkQAAAIA5jA9tKSkpioyMdLYLCws9PrbiIiYV6whUlhhqAwAAAExjfGgLCgrSyJEjne2ff/7Zo+MyMzOVmZnpbLdq1aq+m+YzrEMCAAAAmMP40CZJV1xxhfN6y5YtSk1NPe4xS5cudVuFcfDgwb5oWv06NtBGZgMAAADM0SRC2/jx450l/m3b1gMPPFBr+eLiYj3yyCPOdmRkpC688EKftrE+MDkSAAAAME+TCG1xcXG67777nO1XXnlFjz/+eLVli4qKNGXKFH3//ffOvltvvbXKKpSBjOe0AQAAAOYI8XcDGsott9yiJUuWaPHixZKkP/zhD3r33Xd17bXXqmvXriopKdGGDRs0e/Zsbdu2zTnu9NNPP+7IXKCwWPMfAAAAME6TCW3BwcF66623NGbMGH3yySeSpK+++kpfffVVjccMHTpUb7/9dqNYORIAAACAmZrE9MhyUVFRWrp0qV588UV16dKlxnLt27fXk08+qU8//VTJyckN2MITUz7OxuxIAAAAwBxNZqStnGVZuvHGG3XjjTdq/fr12rhxo9LS0iRJiYmJOv3003XyySc3yqmGjbDJAAAAAI6jyYW2ivr166d+/fr5uxn1joE2AAAAwBxNanqk6SwW/QcAAACMQ2gzECNtAAAAgDkIbQaxWIkEAAAAMA6hzSBMjgQAAADMQ2gzEONsAAAAgDkIbSZhqA0AAAAwDqHNQNzSBgAAAJiD0GaQ8iX/yWwAAACAOQhtBrGYHgkAAAAYh9BmIKZHAgAAAOYgtBmEgTYAAADAPIQ2IzHUBgAAAJiC0GYQy2IhEgAAAMA0hDaDsBAJAAAAYB5Cm4FYiAQAAAAwB6HNIAy0AQAAAOYhtBmIgTYAAADAHIQ2gzj3tJHaAAAAAGMQ2ozCBEkAAADANIQ2A9kMtQEAAADGILQZhCX/AQAAAPMQ2gzEkv8AAACAOQhtBmEdEgAAAMA8hDaDWMyPBAAAAIxDaDMQ0yMBAAAAcxDaDMI4GwAAAGAeQhsAAAAABDBCm0HKb2njOW0AAACAOQhtBrGYIAkAAAAYh9BmIBYiAQAAAMxBaDMIK/4DAAAA5iG0GYiBNgAAAMAchDYAAAAACGCENhMx1AYAAAAYg9BmkF+X/AcAAABgCkKbQVjyHwAAADAPoc1ANmv+AwAAAMYgtBmEJf8BAAAA8xDaDMQ4GwAAAGAOQptBWIgEAAAAMA+hzSAsRAIAAACYh9BmIobaAAAAAGMQ2gzCQiQAAACAeQhtBmKgDQAAADAHoc0g5QNtPKcNAAAAMAehzSBMjwQAAADMQ2gzEONsAAAAgDkIbUZhqA0AAAAwDaHNRAy1AQAAAMYgtBmk/J42MhsAAABgDkKbQZgcCQAAAJiH0GYgVvwHAAAAzEFoM4jFmv8AAACAcQhtBrK5qw0AAAAwBqHNIOXjbEQ2AAAAwByENoMwOxIAAAAwD6HNRAy1AQAAAMYgtBnEYtF/AAAAwDiENgMx0AYAAACYg9BmkPJ72nhOGwAAAGAOQhsAAAAABDBCm4F4ThsAAABgDkKbQVjyHwAAADAPoc1EDLQBAAAAxiC0GcQ6NtRGZgMAAADMQWgzCLMjAQAAAPMQ2gzESBsAAABgDkKbQViIBAAAADAPoc1APFwbAAAAMAehzSCWyhciIbUBAAAApiC0GYTpkQAAAIB5CG0mYqANAAAAMAahzSAMtAEAAADmIbQZiIE2AAAAwByENpMcG2pj9UgAAADAHIQ2g1hMkAQAAACMQ2gzEANtAAAAgDkIbQZhyX8AAADAPIQ2AAAAAAhghDaDVBxos1mNBAAAADACoc0gFvMjAQAAAOMQ2gzFQBsAAABgBkKbQRhnAwAAAMxDaDMUA20AAACAGQhtBql4SxsLkQAAAABmILQZxGKCJAAAAGAcQpuhGGcDAAAAzEBoMwkDbQAAAIBxCG2G4pY2AAAAwAyENoO4LUTCBEkAAADACIQ2gzA7EgAAADAPoc1UDLQBAAAARiC0GcSyGGsDAAAATENoMxQDbQAAAIAZCG0GYZwNAAAAMA+hzVAs+Q8AAACYgdBmEJb8BwAAAMxDaDOIxQRJAAAAwDiEtmOuvvpqWZbl9pOamurvZtUZ0yMBAAAAMxDaJH344Yd64403/N2ME8aK/wAAAIB5mnxoy8zM1M033+zvZtQ7BtoAAAAAMzT50Hb77bcrLS1NknTeeef5uTX1x2Z+JAAAAGCEJh3aFi9erFdffVWSNHr0aF111VV+btGJYXokAAAAYJ4mG9qOHj2q//u//5MkxcbG6rnnnvNzi+oX42wAAACAGZpsaPvDH/6gffv2SZIeeeQRtW/f3s8tOnEs+Q8AAACYp0mGtqVLl+qVV16RJA0dOlS33HKLn1vkAwy1AQAAAEZocqEtKytLN954oyQpPDxcL730kixDbgar+DHIbAAAAIAZmlxo++Mf/6g9e/ZIkmbMmKHu3bv7uUX1x4zoCQAAAKCiEH83oCEtX75cL774oiTp1FNP1Z/+9Kd6qzs3N7fa/dHR0fV2Dm+w5D8AAADgG9X93b+mPFAfmkxoy8nJ0dSpUyVJwcHBeumllxQSUn8fPzk5udr9paWl9XaO46kY1Fwul1wuV4OdG42Xy+WSbdv0F3iMPgNv0WfgLfoMvNXQfSYmJqZBzlOuyYS2u+66S6mpqZKk3//+9zrjjDMa5Lzp6ekNch5JKnX9GtoOHsxQSV5Yg50bjZfL5dLRo0dl27aCgprcjGnUAX0G3qLPwFv0GXjL9D7TJELbihUrnOewpaSk6P7776/3c6SlpVU7FbIhp0e6KoS2hJYt1TI2osHOjcbL5XLJsiwlJiYaeZFD/aPPwFv0GXiLPgNvNXSfycrKqrIvNzdXrVu39sn5jA9teXl5uuGGG5ypg7Nnz1ZkZGS9nyc2NtZv96+Vs6xfQ1tQUBAXOXjMsiz6DLxCn4G36DPwFn0G3mrIPhMbG1tlny/Pa/z/BXfffbd+/vlnSdLUqVN11lln+blFDYOFSAAAAAAzGB3aNm3apKefflqS1Lp1az366KN+bpFvmfK8OQAAAAC/Mnp6ZHp6ujPilJaWpvj4eK+O79y5s/O6WbNmOnLkSH02z6cYZwMAAADMYPRIW1PG7EgAAADADEaPtIWGhiohIcHj8oWFhcrJyXG24+PjnRsKmzVrVu/t8wXLIrABAAAAJgmY0Gbbtr744gt9//33KioqUqdOnXTBBRec0IPrhgwZooyMDI/Lz507V1OmTHG2161bp06dOtX5/AAAAABwonwW2nbu3Kk1a9ZIkqKionTxxRfXWPaHH37QxIkTtXXrVrf9kZGRevjhh/Xb3/7WV800jiXuZwMAAABM4rN72h544AFdddVVuuqqq7Rw4cIay+3evVsjR47U1q1b3Zapt21beXl5uv322/Xggw/6qpnGYsl/AAAAwAw+C22LFi1ygsP1119fY7k//OEPzqqMlmXJtm3nuPLtmTNnasOGDb5qqlHKl/0nsgEAAABm8Elo27lzpw4ePCjLspSQkKABAwbUWO69995zgkafPn20cOFCbdq0Sc8995xiYmJkWZZcLpceeughXzTVODypDQAAADCLT+5p27Jli/P6tNNOq7Hc/PnznVG1pKQkffHFF4qNjZUk9ejRQ0lJSRo/frykspG7vLw8RUVF+aLJxmF2JAAAAGAGn4y07d6923ndvXv3Gst98sknksqm9F1//fVOYCs3duxYdenSRVLZcvzr16/3QWt/NXnyZGd6pm3bjXLlSIuhNgAAAMAoPgltWVlZzuuanm9WXFys1atXO9vjxo2rttzQoUOd1xVH8FA7m7vaAAAAACP4JLQVFRU5r4ODg6sts27dOhUUFEiS4uLidMYZZ1Rbrl27ds7r8gVLULPygTamRwIAAABm8Eloi46Odl5nZmZWW2blypWSyqZGnnnmmTXWVTH0FRYW1lMLDcb8SAAAAMAoPgltycnJzutNmzZVW2bZsmXO69pCW8XRNRYh8RwDbQAAAIAZfBLa+vTpI6nsAc9fffWVMjIy3N5PS0vTp59+6myPGDGixrp27drlvK4YBlE9xtkAAAAAs/gktPXu3VsdOnSQZVkqLCzU9ddfr7y8PElSQUGBbrzxRpWUlEiSWrZsqSFDhtRY17p165zXKSkpvmiumRhqAwAAAIzgk9AmSTfddJPzDLZFixapXbt2Gjx4sNq2baslS5ZI+nWp/5oWK9m+fbvz+ICQkBCdcsopvmquMcpvaWP1SAAAAMAMPgttd9xxh/r06eMEtyNHjuibb75xW5ikTZs2uuuuu2qs45133pFUFu5OPfVURUZG+qq5xrCYIAkAAAAYxWehLSwsTMuWLdOIESOc4Fbxv506ddKiRYvUvHnzao+3bVuzZ892ts8//3xfNdVILPkPAAAAmCHEl5UnJSXps88+05dffqn//ve/OnDggGJiYjRgwACNGTNGYWFhNR67fv16tW/fXu3bt5ckXXHFFb5sqjFY8R8AAAAwi09DW7lhw4Zp2LBhXh1z2mmn6bPPPvNRi8zHQBsAAABgBp9Nj4R/lA+02cyPBAAAAIxAaDMM0yMBAAAAszTI9EhPFRYWauvWrSoqKlLHjh2VmJjo7yY1WoyzAQAAAGbw2UhbUVGR0tPTlZ6eroyMjFrLZmdn6//+7//UokUL9evXTwMHDlSrVq00YsQIt4drwxMMtQEAAAAm8Vlou/fee9W6dWu1bt1akydPrrFcfn6+RowYoZdffln5+fmybdv5+fLLL3XmmWdq6dKlvmqmsbilDQAAADCDz0Lb+++/7yyGccstt9RY7r777tOGDRsklT1EuyLLslRcXKwJEyYoPT3dV001SvlXSGYDAAAAzOCT0JaRkaFt27bJsixFRkbqnHPOqbbckSNH9MwzzzhhLSYmRnfeeaeeffZZTZgwQbZty7IsZWdn6+9//7svmmocJkcCAAAAZvHJQiT/+9//nNf9+vVTeHh4teXefvtt5efnS5IiIiK0cuVKnXzyyZKkm2++Wb169dJ9990nSXrjjTf02GOPKSiIBS89wvxIAAAAwAg+SUCpqanO6169etVYbvHixZLKpkFOnDjRCWzl7r77brVs2VKSdOjQIf3444/131jDVJ5iCgAAAKBx80loO3z4sPM6ISGhxnJffPGF83rChAlV3g8NDdVZZ53lbFccwUPtGGgDAAAAzOCT0FY+5VGSIiMjqy2zZcsWJ9yFhoZq5MiR1ZY76aSTnNcHDx6sv0YaqnycjcwGAAAAmMEnoS0iIsJ5nZ2dXW2ZlStXSiqbznfGGWfUeN9bxdCXl5dXj600E7MjAQAAALP4JLS1aNHCeb1jx45qy3z66afO6zPPPLPGuiqGvpqCHaqymR8JAAAAGMEnoa188RHbtvX555+rqKjI7f38/HwtWrTI2R42bFiNde3fv995XTEMonoMtAEAAABm8Ulo69u3r+Li4mRZljIzM/XQQw+5vf/QQw8pKytLUtnoWcXFRiorf/C25H5/G2rHOBsAAABgBp88py08PFwTJ07Uiy++KMuy9OCDD2rVqlU644wztH79ei1btkxS2f1s48aNU2xsbLX1HDp0SFu2bHG2Kz8SAFWVL/nP7EgAAADADD4JbZI0c+ZMvfXWWzpy5Igkafny5Vq+fLlbmfDwcN1777011vHBBx+otLRUlmUpJSWl1scHAAAAAICJfDI9UpKSk5O1cOFCtWjRQrZtV/kJDQ3VSy+9pO7du9dYx9y5c52Ro3POOcdXTTUSA20AAACAGXw20iaVrQq5ZcsWPfPMM1q+fLkOHDigmJgYDRgwQNOnT3cWLKnOmjVr9NVXXznbl1xyiS+bagyW/AcAAADM4tPQJkkJCQmaMWOGZsyY4dVx/fv3l8vl8lGrzMeS/wAAAIAZfDY9Ev5hMdQGAAAAGIXQBgAAAAABjNBmmPJxNmZHAgAAAGbw+T1tFZWWlurbb7/V119/rS1btigzM1PZ2dmKjY1VfHy8evToocGDB2vAgAEKDg5uyKYZg9mRAAAAgFkaJLTl5eXp8ccf13PPPae0tLTjlm/Tpo2mTZum2267TVFRUQ3QQvPYLPoPAAAAGMHn0yPXrl2rfv36acaMGfrll1+Ou6qhbdvat2+f7rnnHp122mlat26dr5toFAbaAAAAALP4NLStW7dOo0aN0vbt22XbtrOyYfkDtiU5I2kV91mWJdu29dNPP+mss87S+vXrfdlMI3FPGwAAAGAGn4W27OxsXXzxxcrOznb2RUVF6YYbbtCiRYuUlpamkpISZWdnq6SkRGlpaVq0aJGmTp2q6OhoSWXhrbyenJwcXzXVKE4w9nM7AAAAANQPn4W2Rx55RPv373dGzc477zxt2bJFs2fP1oUXXqjk5GQnYFiWpeTkZF144YV68cUXtWXLFl1wwQXOyNv+/fv1yCOP+KqpAAAAABCwfBLabNvW7NmznVA2evRoLVy4UG3btvXo+DZt2uijjz7S6NGj3eqD55geCQAAAJjBJ6Htu+++U0ZGhmzbVnh4uF5++WWFhHi3UGVwcLBeeuklRURESJIyMjK0Zs0aXzTXKCz5DwAAAJjFJ6Ft8+bNksqmPZ5//vlKSkqqUz3Jyck6//zzq9SL42PJfwAAAMAMPglt6enpzusePXqcUF0Vjz948OAJ1dUUWOWL/pPZAAAAACP4/Dltx3suG+oX0yMBAAAAs/gktFWcDrl169YTqmvLli3O68TExBOqqykhKgMAAABm8EloK5/SaNu2li1bpoyMjDrVc/DgQS1btqxKvagZA20AAACAWXwS2vr376+EhARZlqWCggLddNNNXk+TdLlcuummm5Sfny9JatGihQYMGOCL5hqJaakAAACAGXwS2izL0pQpU5zg8P7772v8+PEeLyRy8OBBXX755frggw+c+m644QZfNNU4FuuQAAAAAEbx2UIk99xzjxISEiSVjfp88MEH6tatm37729/qk08+qTJlMiMjQ5988ommT5+ubt26OYFNklq2bKk///nPvmqqUSwmSAIAAABG8e6J115o1qyZPvzwQ5177rnOFMejR4/q2Wef1bPPPitJCgoKUlRUlPLy8uRyuZxjbduWZVmybVtRUVH68MMP1axZM1811UjMjgQAAADM4NMl/88880wtXbpU7du3d4KYVBbKbNtWaWmpsrOzVVpa6uyT5AS2jh07atmyZRo4cKAvm2kWBtoAAAAAo/j8OW1DhgzRDz/8oHvvvVdJSUnHXSDDtm0lJSXpvvvu0w8//KDBgwf7uolGYqANAAAAMIPPpkdWFBcXp5kzZ+qvf/2rVq9erVWrVmnr1q3KzMxUdna2YmNjFR8fr+7du2vw4MEaOHCgQkNDG6JpxikfaGP1SAAAAMAMDRLayoWGhmrYsGEaNmyYx8c8+OCDzv1uM2bM8FXTjFE+BRUAAACAGRo0tNXFzJkzCW11wEAbAAAAYAaf39NWH5jq5znG2QAAAACzNIrQBgAAAABNFaHNMOW3tDE6CQAAAJiB0GYYpkcCAAAAZiG0GYpxNgAAAMAMhDbDsOQ/AAAAYBZCm6G4pQ0AAAAwA6HNMOXjbGQ2AAAAwAyENtMwOxIAAAAwCqHNUCz5DwAAAJiB0GYYFiIBAAAAzEJoMxTjbAAAAIAZQup64KhRo+qzHTUqLS1tkPOYwlmIhNQGAAAAGKHOoW3FihUNMhXPsizuz/ICsyMBAAAAszA90lgEXQAAAMAEdR5pk1ihMBAx0AYAAACYpc6hbefOnfXZDtQz8jQAAABghjqHto4dO9ZnO1BPyu8zJLMBAAAAZuCeNsMwPRIAAAAwC6HNUEyPBAAAAMxAaDMNQ20AAACAUQhthrK5qw0AAAAwAqHNMNaxoTamRwIAAABmILQZxmJ6JAAAAGAUQpuhGGgDAAAAzEBoMwwDbQAAAIBZCG2m4qY2AAAAwAiENsOU39NGZAMAAADMQGgzjMUESQAAAMAohDZDMTsSAAAAMAOhzTAs+Q8AAACYhdBmKAbaAAAAADMQ2gxTPtBmMz8SAAAAMAKhzTTMjwQAAACMQmgzFANtAAAAgBkIbYZhnA0AAAAwC6HNUAy0AQAAAGYgtBmm/JY2FiIBAAAAzEBoAwAAAIAARmgzDPe0AQAAAGYhtBnGYsl/AAAAwCiENkNxSxsAAABgBkKbYRhnAwAAAMxCaDMUA20AAACAGUL83YCGUlhYqJUrV2rFihVat26dNm3apIMHD6qwsFDNmjVTu3btNHDgQI0bN07nnntuo703jCX/AQAAALMYH9oOHDig22+/XYsWLVJ2dna1ZTIyMpSRkaENGzbohRdeUO/evfXyyy9r4MCBDdza+tA4wyYAAACA6hk/PXLPnj2aP39+lcDWunVr9e/fX6NGjVKvXr0UFPTrV7Fx40YNHTpU7777bkM3t94wzgYAAACYwfjQVtGgQYP0/PPPa+fOnfrll1/07bffavny5dq4caP27dun6dOnO9MiS0pKdNVVV2nr1q1+brV3GumsTgAAAAA1MD60BQUF6bLLLtPatWu1atUq3XTTTerUqVOVcq1atdJTTz2lJ554wtlXVFSke+65pwFbW3+4pQ0AAAAwg/Gh7bTTTtP777+v0047zaPyv/3tbzVgwABne9GiRcrLy/NV8+qdsxAJEyQBAAAAIxgf2urisssuc14XFBQoNTXVf43xErMjAQAAALMQ2qrRokULt+2srCw/teQEMNAGAAAAGIHQVo3KI2tJSUn+aUgdNNbnywEAAACoHqGtEtu29fbbbzvbrVu3VufOnf3YorphoA0AAAAwg/EP1/bWG2+8oR07djjbV199tUejV7m5udXuj46Orre2eaK8paweCQAAAPhGdX/3rykP1AdCWwV79+7V7373O2e7efPm+vOf/+zRscnJydXuLy0trZe2ecu2XXK5XH45NxoXl8sl27bpL/AYfQbeos/AW/QZeKuh+0xMTEyDnKccoe2YvLw8jRs3TocOHXL2vfDCC1UWJfFWenr6iTbNK0VFhZKko1lZDX5uNE4ul0tHjx6VbdsKCmLGNI6PPgNv0WfgLfoMvGV6nyG0SSopKdHEiRO1Zs0aZ9+0adN05ZVXelxHWlpatVMhG3p6ZHj4LknZiouNa1QLqMB/XC6XLMtSYmKikRc51D/6DLxFn4G36DPwVkP3mepWl8/NzVXr1q19cr4mH9pcLpeuvfZaffTRR86+K6+8Uk888YRX9cTGxjZ4QKvesbvaLIuLHDxmHesv9Bl4ij4Db9Fn4C36DLzVkH0mNja2yj5fnrdJ/1/gcrk0efJkzZ8/39k3fvx4vf766woODvZjy+qufM0U1iEBAAAAzNBkQ5vL5dINN9ygefPmOfvGjh2r+fPnKySk8Q5A8pQ2AAAAwCxNMrS5XC5NnTpVc+fOdfaNGTNGb775ZqMObBXZrPkPAAAAGKHJhbbywDZnzhxn35gxY7RgwQKFhob6sWX1w4NHygEAAABoRJpUaDM9sFXEOBsAAABghiYT2qoLbGPHjjUusFliJRIAAADAJE0itNm2rRtvvNEtsI0bN05vvvmmUYFNkoKOZTYX97QBAAAARjA+tNm2rZtuukmvvPKKs+/yyy83MrBJUvCx1FbqIrQBAAAAJjBjqcRavPXWW5o9e7azbVmWMjMzdfHFF3tcxx133KFzzz3XF82rd4Q2AAAAwCzGh7a8vDy3bdu2tXz5cq/qmDhxYn02yaeCji0fWcr0SAAAAMAIxk+PbGrKR9pcLj83BAAAAEC9MH6kbfLkyZo8ebK/m9FgnOmRjLQBAAAARmCkzTDloa2Ee9oAAAAAIxDaDBNslU+PJLQBAAAAJiC0GSaI1SMBAAAAoxDaDOOMtHFPGwAAAGAEQpthgoO5pw0AAAAwCaHNMNzTBgAAAJiF0GaYoGO/UZb8BwAAAMxAaDNM+UgbC5EAAAAAZiC0GSaE1SMBAAAAoxDaDMOS/wAAAIBZCG2G+XXJfz83BAAAAEC9ILQZhpE2AAAAwCyENsNwTxsAAABgFkKbYZyRNpb8BwAAAIxAaDMMS/4DAAAAZiG0GSaY6ZEAAACAUQhthiG0AQAAAGYhtBmmPLS5uKcNAAAAMAKhzTDHMhsjbQAAAIAhCG2GYXokAAAAYBZCm2Gc1SOZHgkAAAAYgdBmGOeeNkbaAAAAACMQ2gzD9EgAAADALIQ2wwTxcG0AAADAKIQ2wzgjbdzTBgAAABiB0GaYX6dH+rkhAAAAAOoFoc0wPFwbAAAAMAuhzTDl97SVcE8bAAAAYARCm2FY8h8AAAAwC6HNMCEs+Q8AAAAYhdBmmCBWjwQAAACMQmgzTHBZZmOkDQAAADAEoc0wQUyPBAAAAIxCaDNMsMWS/wAAAIBJCG2GCWakDQAAADAKoc0whDYAAADALIQ2w5RPjyS0AQAAAGYgtBmmfCES7mkDAAAAzEBoM8yvD9f2c0MAAAAA1AtCm2F+XfKf1AYAAACYgNBmGOeeNmZHAgAAAEYgtBmG1SMBAAAAsxDaDFMe2iTJRXADAAAAGj1Cm2EqhrYSQhsAAADQ6BHaDFMhs7HsPwAAAGAAQpthKo60cV8bAAAA0PgR2gzjFtoYaQMAAAAaPUKbYcqX/JekUtb9BwAAABo9QpthGGkDAAAAzEJoM4xlWc5iJCWMtAEAAACNHqHNQOEhZb/WwpJSP7cEAAAAwIkitBmoPLQVFLv83BIAAAAAJ4rQZqDwkLL5kQXFjLQBAAAAjR2hzUC/jrQR2gAAAIDGjtBmoIhjoS2f0AYAAAA0eoQ2A3FPGwAAAGAOQpuBWD0SAAAAMAehzUDc0wYAAACYg9BmoF9Xj2R6JAAAANDYEdoMxEIkAAAAgDkIbQZieiQAAABgDkKbgVg9EgAAADAHoc1AjLQBAAAA5iC0GYgl/wEAAABzENoMVB7a8osIbQAAAEBjR2gzEEv+AwAAAOYgtBmofMn/AqZHAgAAAI0eoc1ALEQCAAAAmIPQZiDnnjamRwIAAACNHqHNQNFhwZKk7IJiP7cEAAAAwIkitBmoRVSIJOlQTpGfWwIAAADgRBHaDNQiKlSSdDS/WEUlTJEEAAAAGjNCm4FiI4IVHFS27P/hXEbbAAAAgMaM0GagIMtSi+gwSVJGTqGfWwMAAADgRBDaDNUyhtAGAAAAmIDQZqgEZ6SN6ZEAAABAY0ZoM1RCTLgkRtoAAACAxo7QZqg2zSIkSbsP5/m5JQAAAABOBKHNUD1axUqStqRl+bklAAAAAE4Eoc1Q5aFt6/5suVy2n1sDAAAAoK4IbYbq3DJaYcFByi0qZYokAAAA0IgR2gwVEhykU9o1kyR9uT3Dz60BAAAAUFeENoOd0zNZkrRs434/twQAAABAXRHaDHZ+77LQtnJ7hvYwRRIAAABolAhtBjspMUZDu7SUy5Ze+GKHv5sDAAAAoA4IbYabdlYXSdIb3+zWj3uP+rk1AAAAALxFaDPcmSkJuvTUNnLZ0l/e+1FFJS5/NwkAAACAFwhtTcBfR/dUbESIftx3VPd9uFG2zXPbAAAAgMaC0NYEJMVF6MmJ/WRZ0n++3a1nPtvu7yYBAAAA8BChrYk4q0eS7rmopyTpn8t+0qyPt8jlYsQNAAAACHSEtiZk6rCTdPeFPSRJz63YoRtf/U4ZOYV+bhUAAACA2hDampibR6Ton1ecqrCQIC3fkq5zHvtc76zdy31uAAAAQIAitDVBl5/eTu/dOlg9W8fpSF6x7njre13y9Ff6dMsBwhsAAAAQYAhtTVTvNs304fQh+tMF3RUdFqz/7cvS9XO/03mPf6F5q1KVU1ji7yYCAAAAEKGtSQsNDtKtI7voy7tG6abhJykqLFjb0nN07wcbNeCh/+p389frk00HVFhS6u+mAgAAAE1WiL8bAP9rER2mP1/UU9NGddG7a/fq1dW79PPBXH2w4Rd9sOEXxUaEaFSPJI3snqjhXROVEBPu7yYDAAAATQahDY64iFBNHtJZkwZ30vo9R7Tw+zQt+vEXHcgqdAKcZUl92jbTsK6JGnhSC53eMV5RYXQjAAAAwFf42zaqsCxLp3WI12kd4vXX0T21dnemPt2SrhVbD2pzWpa+33tU3+89qqc/k0KCLJ3ctpkGdm6hAZ1b6LQO8YqPDvP3RwAAAACMQWhDrYKCLPXv1EL9O7XQXRf00IGsAn2+9aBW/3xI3+w8rH1H8rVhzxFt2HNEL3zxsySpQ4sondq+uU5t10x92zdX7zbNFBkW7OdPAgAAADROhDZ4JTkuQlf2b68r+7eXJO3NzNO3Ow+X/aQe1s8Hc7X7cJ52H87TR9//IkkKDrLUPTlWp7Zvpl5tmqlX6zj1aBWr6HC6HwAAAHA8TfJvzfv27dNrr72mDz/8UKmpqcrIyFDLli3VqVMnXXrppbrmmmvUtm1bfzezUWgXH6V28VEad1o7SdLR/GL9uPeovt97xBmBO5hdqE1pWdqUliVpjyTJsqROCdHq2TpWvVrHqWfrOPVqE6dWcRGyLMuPnwgAAAAILJbdxJ6m/Pzzz+vOO+9Ubm5ujWViYmL0z3/+UzfddFOtdeXm5iomJkaSlJOTo+jo6Hpta124XC6lp6crKSlJQUH+f6KDbdvan1Wg7/cc0fd7j2pzWpY2/ZKl9OzCass3jwpVt+RYdU2KKfs59joxNpww5yOB1mcQ+Ogz8BZ9Bt6iz8BbgdBnfJkNmtRI2/3336/77rvPbV/Xrl3Vpk0b7d27Vzt27JBU9iXffPPNOnjwoP7617/6o6nGsCxLrZtFqnWzSF1wcmtnf0ZOoTanZTkhblNalnYczNWRvGJnumVFcREhToDrcizMdUmKUeu4CAUFEeYAAABgriYT2j744AO3wNarVy/NmzdPp512mrPvu+++03XXXafNmzdLku6991716dNHl156aYO313QtY8I1rGuihnVNdPYVFJdqe3qOtqVna9uBHG1Lz9H29BztOpSrrIISrd2VqbW7Mt3qCQ8JUseEKHVKiFanltHH/hulzi2jlRxLoAMAAEDj1yRCW3Fxse68805nu127dvrqq68UHx/vVu6MM87QV199pT59+mjfvn2SpDvvvFMXXXSRQkKaxFflVxGhwTq5bTOd3LaZ2/6C4lLtzMjVtvQcbTtQHuiytetQngpLXPrpQI5+OpBTTX1B6pQQrY4JUerQIurY/XeRahsfqXbxUYphIRQAAAA0Ak3ib63z58/X9u3bne3HHnusSmAr16JFCz322GOaMGGCJGnbtm2aP3++rrnmmgZpK6qKCA1Wz2OLlVRUUurSviP52pmRq9SMXKUeytPOjFztOpSrPZn5Kih2acv+bG3Zn11tvc2jQtW2eaTaHQtxbZuXBbpWcRFKjotQy5gwhQQzjx4AAAD+1SRC24IFC5zXbdq00dixY2stP27cOLVu3VppaWmSpLfeeovQFoBCgoPUMSFaHROipe7u7xWXurQ3M/9YmMvV3sx87c3M097MfO07kq8jecXOz8ZfsqqtP8gqm8bZqlmEkmIjlBwX7gS65GZloa5lTLjio8IUFkK4AwAAgG8YH9ry8/P1ySefONsXXHDBcac6hoSE6IILLtCcOXMkScuWLVNBQYEiIiJ82lbUn9DgIHVuGa3OLatftSe7oFj7juRrX2a+E+T2ZuZpX2a+DmQV6mBOoUpdttKzC4+tdHm01vPFRoQoITpMCTHhahEdpoToMLU49tMyJlzx0WGKiwhRXGSo4iJCFRcZovAQHjgOAACA4zM+tG3evFmFhb8uLz9kyBCPjhsyZIgT2goKCrR582b169fPJ21Ew4uNCFWPVqHq0Squ2vdLXbYO5RbqwNFCHcgq0P6sAqVnFehAVqH2ZxXoQFaBMnKKdDi3UC5byi4oUXZBiVIP5XnchojQoGMBLrRKoIuNCFV0WLCiwkIUFRasqPAQRYcFKzIsWNFhIYoOD1ZkWIhThpE+AAAAcxkf2jZu3Oi23bVrV4+Oq1xu06ZNhLYmJDjIUlJs2bTIU9SsxnIul62j+cU6lFukw7llIa4szJX9HMot0qGcQmXmFSsrv1hZBcXKLiiRJBUUu1RQXFjjM+u8ERJkKTIsWBGhwQoPCTr2E6zw0F9fh9W0P9hScUGe4pvnKiw4SCHBQQoJshQSHKTQYEshQUEKCbbcXjv7ankvJMhSUJClIMtSsGXJssq+1/LXPHcPAADAM8aHttTUVLftDh06eHRcx44d3bZ37txZa/maHtYdCA/chu8EBVmKjw5TfHSYx8eUumzlFJYoK79YR48Fuaz8kmP/LVZWQdl7+UWlyisuVV5hifKKSpVXVKLcolLlF5Uqt6hEeYWlKip1SZJKXLYz2ld3v5zAsd4LOhbirGOhrux1xWBnKThIFV5XKOOULysTZFmyVBYELUu/vpaObZftDDr2uiw0/vq6rD3VHytZZcdVc2xZnRXP8+uxVd/79diykmUqZteKMdZ9v3WcstUH4Orq8PicFTbcarck27aVl5un6OhM5zv39DPUdn7VcE5P6vGXAGhCYHwPx2mEy7aVm5Oj6JhsBQVCg30kED6aFQC9sj6+B9u2lZOTo5iYnDr9I5//vwU0NNu2NbhdmJKSGuZ81f3dv6Y8UB+MD21ZWe6LTDRv3tyj45o1cx9dyc6ufgXCcsnJydXuLy0t9eh89cXlcsm2bblcrgY9LzxnSYoND1ZseLDaNj+x+ySLS11l4e5YqCsscZX9FJf++rrEpcKSUhUWu1RUWvX9guJSHc3JU3BImEpctkpctopLXSp12SoutVXicqmk1Fbxsf+WlLqcciWlrmNljr122So99nM8LltyldqSjl8WAADgeJ6/opt6NdDfgWNiYhrkPOWMD205Oe7P74qMjPTouMrljhfaapKenl6n4+rK5XLp6NGjsm1bQUHc59RUBEmKkRQTLClYUnj5O9axHTUvelLeZ5o1a1ZvfcZl23K5jv3X/vW/pbYt2y4bbbRVNr3U2Vd+jKo/tqxs2b+kudV5rC7bLv/vsR/9ul3epvLXZe8fK2NX2D5WoPw4V/m2/es+5xin7l+3XZW23c5dYbucXWGH7ba/+u/VvYyXx1bYb1c4wn2/Z8fatq2iwiKFhoVV+y/gtdVZ07k9Od6u6QA/CYTmBEATPGqDbdsqLqq5z5xwGwLhlxEAAuFrqK8mnEifCYTvAf5gK6QkX+np6Ub+Hdj40FZcXOy27elDsiuXq1xPZWlpadVOhWzo6ZEul0uWZSkxMdHIDov6R5+Bt1wulw4ePEifgcfoM/AWfQbeaug+U3k2n1Q2PbJ169Y+OZ/xoa1yaCooKFBUVNRxjysoKKi1nspiY2MD5v41y7IUFBTERQ4eo8/AW/QZeIs+A2/RZ+CthuwzsbGxVfb58rzG/19Qeb5pXp5nS7JXLlfdLwYAAAAAfM340JaYmOi2nZaW5tFxlcu1bNmy3toEAAAAAJ4yPrT16NHDbXvXrl0eHVe5XM+ePeutTQAAAADgKeNDW+/evd22161b59Fxlcv16tWr3toEAAAAAJ4yPrS1b99eKSkpzvbnn3/u0XEVy3Xp0kXt2rWr97YBAAAAwPEYH9okady4cc7rFStWaPfu3bWW3717t1toq3g8AAAAADSkJhHapkyZouDgsocLu1wuPfDAA7WWv//+++U69jT14OBgTZkyxedtBAAAAIDqNInQ1rNnT02aNMnZfumll/TSSy9VW/aFF17Qyy+/7GxPnjy5ymImAAAAANBQjH+4drlZs2bp888/144dOyRJN954oz766CNNnDhRbdq00b59+/Sf//xHCxcudI7p0qWLHnnkEX81GQAAAACaTmhr2bKllixZovPPP187d+6UJH344Yf68MMPqy3fuXNnLVmyhOezAQAAAPCrJjE9slzXrl31ww8/6LbbblNcXFy1ZZo1a6bbbrtNP/zwg7p06dLALQQAAAAAd5Zt27a/G+EPBQUF+vzzz5WamqpDhw4pISFBnTp10siRIxUeHu5RHbm5uYqJiZEk5eTkKDo62pdN9qo9WVlZio2N9Wt7EPjoM/AWfQbeos/AW/QZeCtQ+owvs0GTmR5ZWUREhM4//3x/NwMAAAAAatWkpkcCAAAAQGNDaAMAAACAAEZoAwAAAIAA1mTvaasPFddwyc3N9WNLqrYhNzdXQUFkctSOPgNv0WfgLfoMvEWfgbcCpc9UbEd9r/XYZFePrA/p6elKTk72dzMAAAAABJADBw4oKSmp3urjny4AAAAAIIAx0nYCXC6XMjIyJElRUVGyLMvPLQIAAADgD7ZtKy8vT5LUsmXLep2mSWgDAAAAgADG9EgAAAAACGCENgAAAAAIYIQ2AAAAAAhghDYAAAAACGCENgPs27dPs2bN0pAhQ9S2bVuFh4erbdu2GjJkiGbNmqV9+/b5u4nwUGFhoT799FPNmDFDF198sU466STFxsYqLCxMiYmJ6tevn26++WYtW7aszg9t9GV/oS8GnquvvlqWZbn9pKamelUHfcZspaWl+uSTT3TLLbeob9++atWqlUJDQxUXF6devXppwoQJevbZZ7Vx40aP66TPmKmgoEBvvPGGfvOb36hHjx5q3ry5QkJC1Lx5c/Xo0UNXXXWVXnvtNeXn53td9+HDh/X0009r1KhR6tChgyIiItSqVSv1799fM2bM0LZt2+rcbl/W3ZRkZ2friy++0P/7f/9P1113nXr37q2QkBDnz5ZOnTqd8Dka67WjQa5LNhq15557zo6OjrYl1fgTExNjP//88/5uKmqxf/9+e+LEiXZsbGytv8uKP71797ZXr17t1Xl82V/oi4Hngw8+qPb3sHPnTo/roM+YbeXKlfapp57q8XWnuLj4uHXSZ8y0cOFCu127dh71kzZt2tgffPCBx3W/8847dmJiYq11hoaG2vfff79dUlLiVbt9WXdT0q1bN9uyrFq/x44dO57QORrrtaOhrkuEtkZs5syZVTpF165d7REjRtgpKSlV3nvggQf83WTUYM2aNdX+T966dWu7f//+9qhRo+xevXrZQUFBbu+HhITY77zzjkfn8GV/oS8GnsOHD9utW7c+odBGnzHb888/X+UvYXFxcXbfvn3tUaNG2YMGDbKTkpK8Cm30GTPNmzevyp8/kZGR9hlnnGGfffbZ9umnn25HRES4vW9Zlv3KK68ct+5///vfVX53HTp0sEeMGGH37NmzSh+dOnWqx+32Zd1NTW2BpD5CW2O9djTkdYnQ1ki9//77bp2gV69e9tq1a93KrFmzxu7Zs6dbOW/+5QsNp2JoGzRokP38889X+xfrtLQ0e/r06W5/0ISFhdlbtmyptX5f9hf6YmC67rrrnO/6vPPO8zq00WfM9uKLL7pdRwYNGmQvW7bMLioqqlJ2165d9nPPPWf379+/1pEI+oyZdu3aZUdFRbmFtSeeeMLOy8tzK5ebm2s/9thjbuEtMjLS3rFjR411f/fdd3ZoaKhTvm3btvZ///tftzJbtmyxhw4d6vZ7ffLJJ4/bbl/W3RSVfz/R0dH24MGD7d/+9rf2nDlz7AsuuOCEQ1tjvXY09HWJ0NYIFRUV2V26dHF++e3atbMPHz5cbdlDhw7Zbdu2dUv/nkxvQcNau3atfdlll1X5n70mTz75pNsFYPz48TWW9WV/oS8GpkWLFjnf8+jRo+05c+Z4FdroM2bbvHmz21+sp02bZrtcrhOqkz5jrr/85S9u14+333671vJvvvmmW/k//elPNZYdMWKEUy42NrbGgFdQUGD37dvXKRsfH19jH2iIupui1157zd60aZNdWlrqtn/SpEknFNoa67XDH9clQlsj9Oqrr7pdEBcsWFBr+coX0Hnz5jVQS+FLAwYMcH6nERERdm5ubrXlfNlf6IuB58iRI84fDrGxsfbu3bu9Dm30GbMNGTLE+T7PPffceqmTPmOuiv2lV69eHh1TcWRh8ODB1Zb54osv3H5P//jHP2qt85tvvnErX9s0M1/WDXcnGtoa67XDH9clQlsjdPHFFzu/9DZt2hw3rRcXF7vd23LppZc2UEvhSw899JDbBWDjxo3VlvNlf6EvBp7rr7/e+X6feeYZ27Ztr0MbfcZcq1atcusLx5ta7Sn6jLm6du3qfJcTJ0706JgJEyY4x3Tr1q3aMtOnT3f7h0dPRrf69+/vHNOnT58ay/mybrg70dDWWK8d/rguseR/I5Ofn69PPvnE2b7gggsUEhJS6zEhISG64IILnO1ly5apoKDAZ21Ew2jRooXbdlZWVpUyvuwv9MXAs3TpUr3yyiuSpKFDh+qWW27xug76jNmef/555/WQIUPUvXv3E66TPmO22NhY57Wn32PFcvHx8dWW+fDDD53XgwcPrrFcRRdffLHz+ocffqjx8SW+rBv1p7FeO/x1XSK0NTKbN29WYWGhsz1kyBCPjqtYrqCgQJs3b673tqFhVf4DJSkpqUoZX/YX+mJgycrK0o033ihJCg8P10svvSTLsryuhz5jto8//th5fd5559VLnfQZs5155pnO66+//lpFRUW1li8sLNTXX3/tbA8fPrxKmcOHD2v37t3Odl1+r5K0fv36Bq0b9auxXjv8dV0itDUylR9u2rVrV4+Oq1xu06ZN9dYmNDzbtvX22287261bt1bnzp2rlPNlf6EvBpY//vGP2rNnjyRpxowZdR5Boc+YKzU1VQcOHHC2Tz31VEnS/v379fe//139+/dXy5YtFRERobZt2+qcc87RP/7xDx06dKjWeukzZrv11ludUYT09HTdc889tZb/85//rIMHD0qSYmJiNH369Cpl6DOQGm8/8FcfI7Q1MpVHVzp06ODRcR07dnTb3rlzZ301CX7wxhtvaMeOHc721VdfXe2oii/7C30xcCxfvlwvvviipLK/iP/pT3+qc130GXNt2LDBbbtNmzZ67bXX1KNHD91zzz367rvvdOjQIRUWFuqXX37R8uXLddddd6lz58569tlna6yXPmO2Xr166fHHH3e2//nPf2r06NFaunSpDh8+rNLSUh06dEhLlizRBRdc4JSNjY3VW2+9Ve3vrK6/17Zt2yo4ONjZrs8+40ndqF+N9drhr+tS7RMwEXAq37fUvHlzj45r1qyZ23Z2dnZ9NQkNbO/evfrd737nbDdv3lx//vOfqy3ry/5CXwwMOTk5mjp1qiQpODhYL7300nHn1teGPmOujIwMt+333ntPDz/8sLPdpk0bdenSRcXFxdq0aZOOHj0qqez7njZtmnbv3q1HHnmkSr30GfNNnz5d7dq10+9//3ulpqZq8eLFWrx4cbVlg4ODddFFF+nhhx9W7969qy1T199rcHCwYmJi3PpmQ9aN+tVYrx3+ui4x0tbI5OTkuG1HRkZ6dFzlclyMGqe8vDyNGzfObbrSCy+8UGVRknK+7C/0xcBw1113Of/q9/vf/15nnHHGCdVHnzHXkSNH3LbLA1u3bt20fPly7du3T59//rm+/vprZWRkaO7cuW5/yZg1a5beeeedKvXSZ5qGMWPG6LPPPtPo0aNrLXf22WfrlltuUa9evWosU9ffa+Wy9dlnPKkb9auxXjv8dV0itDUyxcXFbtue/ot65XKV60HgKykp0cSJE7VmzRpn37Rp03TllVfWeIwv+wt90f9WrFih5557TpKUkpKi+++//4TrpM+Yq7qVyjp06KCVK1dq1KhRbvtDQkI0adIkffLJJwoLC3P233333SotLXUrS58xX25urqZPn65u3bpp0aJFkqSwsDD169dPo0aNUv/+/RUVFSWpbFW8iy66SAMHDtRPP/1UbX11/b1WLluffcaTulG/Guu1w1/XJUJbIxMdHe22XZfld6urB4HN5XLp2muv1UcffeTsu/LKK/XEE0/Uepwv+wt90b/y8vJ0ww03yLZtSdLs2bO9+hflmtBnzFXd9/bYY4+pZcuWNR7Tv39/t4Uktm/frs8++6zWeukzZsnNzdU555yjZ555RsXFxYqKitLjjz+uzMxMrVu3TsuXL9e3336rI0eO6N///rfTn9asWaOhQ4dq27ZtVeqs6++1ctn67DOe1I361VivHf66LhHaGpmYmBi37by8PI+Oq1yu4nNXENhcLpcmT56s+fPnO/vGjx+v119/3e2m6er4sr/QF/3r7rvv1s8//yxJmjp1qs4666x6qZc+Y67K31uzZs00ZsyY4x43ZcoUt+0VK1a4bdNnzHbHHXdo9erVkspG15YtW6bbb7/dGVkrFxoaquuuu04rV650not28OBBXXvttVXqrOvvtXLZ+uwzntSN+tVYrx3+ui4R2hqZxMREt+20tDSPjqtcrrZ/WUXgcLlcuuGGGzRv3jxn39ixYzV//nyPhuN92V/oi/6zadMmPf3005LKHvfw6KOP1lvd9BlzVf7++/bte9x/+JGk3r17KyIiwtmuuHJtdfXSZ8yRlpaml19+2dm+8cYbj/tMqm7durktjvXNN9/oiy++cCtT199rVlaW219867PPeFI36ldjvXb467pEaGtkevTo4ba9a9cuj46rXK5nz5711ib4hsvl0tSpUzV37lxn35gxY/Tmm296PH/al/2Fvug/6enpzrTItLQ0xcfHy7KsGn8qj5R07tzZea/yqlf0GXNVXhgiISHBo+Msy3Jb7Ojw4cNu79NnzLV8+XKVlJQ422PHjvXouMojuJ9++qnbNn0GUuPtB/7qY4S2Rqby8rnr1q3z6LjK5Wpb1Qn+Vx7Y5syZ4+wbM2aMFixYoNDQUI/r8WV/oS+aiT5jrpSUFLf7HgsLCz0+tuK9GJXvnaTPmGvPnj1u2+3bt/fouMrPrao8wtClSxe3BW7q8/fqy7pRvxrrtcNf1yVCWyPTvn17paSkONuff/65R8dVLNelSxe1a9eu3tuG+lFfgU3ybX+hL/pPaGioEhISPP6pPP8+Pj7e7f2K6DPmCgoK0siRI53t8nsijyczM1OZmZnOdqtWrdzep8+YKzw83G07Pz/fo+Mq37tT+f63kJAQDR061Nmuy+81Jiam2kec+LJu1K/Geu3w13WJ0NYIjRs3znm9YsUK7d69u9byu3fvdusoFY9HYKkusI0dO7ZOga2cL/sLfdE/hgwZooyMDI9/nnrqKbfj161b57xX+f4kiT5jsiuuuMJ5vWXLFucZf7VZunSpMx1XkgYPHlylDH3GTG3atHHb/vbbbz06rnK56v5yOn78eOf1zz//rJUrV9ZaZ25urttzAi+66CK3ey0bqm7Ur8Z67fDLdclGo7Np0yY7ODjYlmRLsqdOnVpr+RtuuMEpGxwcbG/evLmBWgpvuFwu+/rrr3d+V5LscePG2UVFRSdUry/7C32xcZgzZ45bv9q5c2et5ekz5jp69KjdsmVL5zu9/vrray1fVFRkn3rqqU75yMhIOz09vUo5+oyZ0tLSbMuynO/z5JNPPu6fSS6Xyz7nnHPcrjnr1q2rUu7gwYN2bGysU+bcc8+ttd4HHnjArc5ly5bVWNaXdcPdpEmTnO+tY8eOXh/fWK8d/rguEdoaqcp/uZ89e3a15Z5//nm3cjfccEMDtxSecLlc9o033uj2u7r88svt4uLieqnfl/2Fvhj4vA1ttk2fMdlTTz3l9r0+9thj1ZYrLCy0r776areyd9xxR4310mfMdOmll7p9p1dccYWdm5tbbdni4mJ72rRpbuUHDBhQY93333+/W9m//vWv1Zb76KOP7NDQUKfc2Weffdx2+7Ju/OpEQ5ttN95rR0NflyzbrjDnAY1GRkaGBg0a5Da16dJLL9XEiRPVpk0b7du3T//5z3+0cOFC5/0uXbpo1apVLGMbgBYsWKAJEyY425ZladSoUR6vEimVPUvn3HPPrfY9X/YX+mLgmzt3rtsKkjt37lSnTp1qPYY+Y67S0lJdeumlWrx4sbNv6NChuvbaa9W1a1eVlJRow4YNmj17ttuDkU8//XR9+eWXNT7EnT5jpu3bt2vAgAFu9zW2bdtWU6ZM0cCBA9W8eXNlZ2dr3bp1mjt3rrZv3+6Ui4yM1Oeff67+/ftXW3dBQYHOOuss5zlwkjR8+HBNmTJFnTp1UkZGht5//33Nnz9fpaWlksqWSV+1apW6dOlSa7t9WXdT9OCDD+rBBx+ssr+4uFgul8vZrnwfpCRde+21mj17do11N9ZrR4Nfl+oU9RAQfvrpJ7tz585u6b2mn86dO9vbtm3zd5NRg8ojIXX5mTNnTq3n8GV/oS8GtrqMtNk2fcZkubm59rnnnuvx9WXo0KH2/v37j1svfcZM33zzjd22bVuv/kxq0aKFvWTJkuPWnZ6ebvfr18+jOhMTE+3Vq1d73G5f1t3U3HfffXX++8mkSZOOW39jvXY05HWJ0NbIZWdn27fddpsdFxdXbQdp1qyZfdttt9nZ2dn+bipq0RChzbZ921/oi4GrrqHNtukzJnO5XPaLL75od+nSpcbrSvv27e0nn3zSq3tr6TNmyszMtO+++247OTm51j+Lmjdvbt922212Wlqax3UXFRXZf/vb3+ykpKRq64yMjLSvu+66au+n9GfdTYmvQ5ttN95rR0Ndl5geaYiCggJ9/vnnSk1N1aFDh5SQkKBOnTpp5MiR1Q5Vo2nzZX+hL5qJPmO29evXa+PGjc7ztBITE3X66afr5JNPlmVZdaqTPmMml8ulTZs2acOGDcrIyFBOTo6ioqKUkJCgPn366JRTTvFqan9FJSUl+uqrr7Rjxw6lp6erWbNmat++vUaOHKnY2NgTarcv60b9aqzXDl9flwhtAAAAABDAeE4bAAAAAAQwQhsAAAAABDBCGwAAAAAEMEIbAAAAAAQwQhsAAAAABDBCGwAAAAAEMEIbAAAAAAQwQhsAAAAABDBCGwAAAAAEMEIbAAAAAAQwQhsAAI3M5MmTZVmWLMtSp06d/N0cAICPEdoAAG5SU1OdQGBZlkaOHOnvJgEA0KSF+LsBAIDGa+7cuUpNTZUkderUSZMnT/ZrexoTvjsAgKcIbQCAOps7d64+//xzSdKIESMIHl7guwMAeIrpkQAANDJz586VbduybdsZrQMAmIvQBgAAAAABjNAGAAAAAAGM0AYAAAAAAYyFSAAAAWXr1q1av369Dh48qJycHCUkJKhjx44aOnSooqOj6+Uctm3r22+/1fbt25WWliaXy6WBAwdqxIgRNR6zc+dObdq0SampqcrKylJISIji4+OVkpKigQMHKioqql7a1hD27NmjVatWKT09XdnZ2WrRooXatWunYcOGKS4urt7Ok56erpUrV2rPnj0qKChQYmKiBg4cqF69etXbOQCgSbABAKhg586dtiTnZ8SIEW7vz5kzx+19T346duxY6znz8vLsWbNm2Z06daqxjrCwMPvKK6+0t2zZ4tHnGDFiRJXPUFJSYs+aNcvu0KFDlfovu+wyt+OLiorsRYsW2ZMnT7bbtm1b6+cLCQmxr7jiCnvDhg21tqm+vrtJkyZ5/N1W9Oabb9p9+vSp8VyhoaH26NGj7fXr13tU32effeZ2/GeffWbbtm3v2rXLHjdunB0aGlrtefr06WMvX77c43YDQFPH9EgAgF+tXbtW3bt311133VXrSohFRUVasGCB+vTpo3//+99en+fIkSMaOXKk7rrrLu3evfu45RctWqTRo0dr7ty52rdvX61lS0pK9NZbb6l///56/vnnvW6br+Xk5OjCCy/UhAkT9MMPP9RYrri4WIsWLdLpp5+uBx98sE7n+uSTT9S3b1+9++67Ki4urrbMDz/8oPPOO0+vvvpqnc4BAE0N0yMBAF4JCgpScHCwJKm0tNTtvfL9lYWEVP/HzaeffqpLL71Uubm5zr6WLVtq2LBhSklJUVRUlA4ePKiVK1c6YaOoqEiTJ0+Wy+XSlClTPGqzbdu65ppr9NVXX0mS2rZtq7PPPlvt2rVTfn6+tmzZotDQ0BqPj4iI0KmnnqoePXooKSlJ0dHRys/P165du5zpf1JZ6LnllluUnJyssWPHVqmnPr87TxUWFurss8/Wt99+67a/b9++Gjp0qJo3b660tDQtXbpUe/fulSS5XC7de++9ysnJ0SOPPOLxuX788Uf95S9/UU5OjkJDQzV8+HD16dNHMTEx2rNnjxYvXqz09HRJZZ//pptu0oABA9SjR48T+owAYDx/D/UBAALL8aZHVlTdFERP7d27127ZsqVzfLNmzewXX3zRLiwsrLb88uXL7fbt2zvlIyMja50qWbFtwcHBtiQ7PDzcfu655+zS0tIq5Suf9+OPP7avvfZae9myZXZ+fn6N53G5XPaHH37oNoUyISHBzs3NrfXzn8h35830yNtvv93t99m2bVv7v//9b5VypaWl9uOPP26HhIQ4ZS3LshcvXlxj3ZWnR4aFhdmS7PPPP99OTU2tUj4nJ8e++uqr3Y65+uqrvfrsANAUMT0SAOAX06dPV0ZGhiQpPj5eX375pW688UaFhYVVW37UqFH68ssvlZiYKEnKz8/XQw895NG5yke13nrrLd18880KCqr6x1/l855//vl69dVXde655yoiIqLGui3L0iWXXKIvvvhCsbGxkqRDhw5p3rx5HrXNl3bs2KEnn3zS2Y6Pj9enn36qs88+u0rZoKAg3X777Xr55ZedfbZt67bbbpNt2x6dr6ioSBdeeKEWLlyojh07Vnk/Ojpac+bMcVuI5J133lFeXp43HwsAmhxCGwCgwW3ZskUffPCBs/3oo4/qlFNOOe5xHTt21MyZM53t//znPzpy5IhH57zqqqt0ySWXeN1WT5100km67rrrnO2FCxf67Fyeevrpp+VyuZztv//97+rWrVutx1x33XW69NJLne3t27dr0aJFHp0vIiJCr7zySq1TOkNDQzVt2jRnu6CgQBs2bPCofgBoqghtAIAGN2/ePGf0Jj4+Xtdee63Hx1555ZWyLEtS2QIg5fepHc8tt9zifUO9VDF4fvPNNz4/3/FUDMbx8fEe3wP4xz/+scZ6ajN+/Hi1atXquOWGDx/utr1582aP6geApoqFSAAADe7zzz93Xg8cOLDGKZHVSUhIUIsWLXTo0CFJ0oYNG3TxxRfXekxERIQGDRpUt8aqbLRpwYIFWrNmjTZt2qTDhw8rOztbJSUlbuUqTiM8ePCgioqKvPps9enAgQPauXOns33++ecrPDzco2OHDh2qxMREHTx4UJL09ddfe3Rc5TBWk06dOrltezpaCgBNFaENANDg1q1b57xeunSp1yskVlx5sTy81eakk06qdXXImuzZs0e/+93v9N5773l9rCRlZmYqOTm5TseeqK1bt7pt9+vXz6vjTzvtNC1dutSpy7ZtZ4SzJm3btvWo7soPSc/JyfGqbQDQ1BDaAAANKi8vT/n5+c62bdtVlr/3xtGjR49bpnnz5l7Xu3XrVo0cOVL79++vQ6vKFBQU1PnYE5WZmem27cm0xZrKl5aWKjs7W3FxcbUeUzmM1aRy+PN0oRMAaKq4pw0A0KDqeypcxYU2auLtKFtpaamuuOIKt8DWsWNHzZgxQ0uXLtW2bdt09OhRFRYWyrZt52fOnDlu9fgzjGRnZ7ttexqoysXExNRaHwCg4TDSBgBoUFFRUW7bEyZM0Pz58/3Umuq98847+vHHH53tK6+8UvPmzTvu/WmBNM2v/PED5So+wNwTlT9L5foAAA2HkTYAQINq3ry52z1s5c9qCyQfffSR8zo2NlYvv/yyRwuKHDhwwJfN8kp8fLzbtrfTPCuWDw4OJrQBgB8R2gAADa5nz57O6/Xr1wfcPU0VF/EYOnRolamCNfnuu+981SSvde/e3W17/fr1Xh1fsXz37t2PuwgJAMB3CG0AgDqreK+YN4uJnH322c7rw4cP64svvqjXdp2oivfdVR6xqklGRoY+++wzj89R1+/OU8nJyTrppJOc7aVLl6qwsNCjY1euXKn09HRne/DgwfXePgCA5whtAIA6qzhlrvJqhbX5zW9+47b9t7/9zaMFRRpKxc+Vmprq0TGPPvqox6Go8jm8+e68cdlll7mdo/JCKTX517/+5bY9duzYem0XAMA7hDYAQJ1VfEjyTz/95NHy+5LUv39/XXjhhc72ihUr9Mc//tGraZIlJSVasWKFx+W90bt3b+f16tWr3RYlqc7SpUurBJ3jqet3541p06YpKOjXP+r/8pe/aNu2bbUe8/rrr7s9l65r165uvysAQMMjtAEA6uzMM890XhcXF+v666/X5s2bPRo1e+aZZ5SQkOBsP/bYY7r44ouPG5C2bdumv//97+ratatuv/32Ore9NhVHllwul8aOHav//e9/VcqVlpbq6aef1mWXXabS0lKvltU/ke/OUykpKbrtttuc7czMTI0aNaraaZwul0tPPfWUpkyZ4rb/iSee4H42APAzlvwHANTZJZdcolatWjkrDb777rt69913FRwcrIiICKdcx44dtXHjRrdjO3furHfeeUcXX3yxs7z84sWLtXjxYp188skaOHCgkpKSFBISoiNHjmj37t1at26d9uzZ49Rx6qmn+uRzjRkzRqeffrrWrl0rSdqxY4dOPfVUnXPOOerXr59CQkK0d+9eLV261PnsycnJuu2223TPPfd4dI4T+e688cgjj+jrr7/Wt99+K0nau3evRo0apdNOO01DhgxR8+bNtX//fn388cdu360k/elPf2KUDQACAKENAFBnERER+s9//qOxY8e6Ld5RWlrq9lywmp4RNmLECH377be6/PLLtWnTJmf///73v2pHtioLDw+ve+NrYVmW3n77bQ0fPtwJMi6XS8uWLdOyZcuqlG/VqpWWLFmiDRs2eHyOE/3uPBUeHq7ly5fr8ssv19KlS53969at07p166o9xrIszZw5U/fee+8JnRsAUD+YHgkAOCEjR47U5s2b9dBDD2nUqFFq3bq1IiMjPT6+Z8+e+v777/Xqq6/qjDPOOO5UvPj4eI0bN06vv/66z+5pk8ruOVu7dq2uu+46t+fKVdS8eXPdcsst+v7779W3b1+vz3Gi352nYmJitGTJEs2fP199+vSpsVxISIguuugirV27lsAGAAHEsgPt4TgAgCbt8OHDWrVqlX755RcdOnRItm0rNjZWbdu2VY8ePdS9e3e3xTUaQkZGhr744gvt2rVLhYWFSk5OVocOHTR06FCfjfb50u7du7Vq1SodOHBAOTk5atGihdq1a6fhw4crLi7O380DAFRCaAMAAACAAMb0SAAAAAAIYIQ2AAAAAAhghDYAAAAACGCENgAAAAAIYIQ2AAAAAAhghDYAAAAACGCENgAAAAAIYIQ2AAAAAAhghDYAAAAACGCENgAAAAAIYIQ2AAAAAAhghDYAAAAACGCENgAAAAAIYIQ2AAAAAAhghDYAAAAACGD/H6OFGWEQ1Yh2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.5\n",
    "plt.tick_params(direction='in', labelsize='26', width=1.5, length=5, top='on',\n",
    "                right='on', zorder=10)\n",
    "plt.plot(range(number_of_epochs), loss_list)\n",
    "plt.xlabel(\"Iteration\", fontsize=28)\n",
    "plt.ylabel(\"Loss\", fontsize=28)\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
